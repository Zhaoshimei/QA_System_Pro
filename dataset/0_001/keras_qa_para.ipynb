{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimei/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_corpus) 43379\n",
      "43379\n",
      "len(data) 43379\n",
      "1000\n",
      "len(del_data) 1000\n",
      "len(vocab) 108425\n",
      "RNN / Embed / Sent / Query = <class 'keras.layers.recurrent.LSTM'>, 50, 100, 100\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "from math import log\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers.embeddings import Embedding\n",
    "from gensim.models import KeyedVectors\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "docs_corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/documents.json\",'r') as docs:\n",
    "    docs_corpus = json.load(docs)\n",
    "            \n",
    "train_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/training.json\",'r') as training:\n",
    "    train_corpus = json.load(training)\n",
    "print(\"len(train_corpus)\",len(train_corpus))\n",
    "\n",
    "test_for_train = train_corpus\n",
    "\n",
    "dev_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/devel.json\",'r') as devel:\n",
    "    dev_corpus = json.load(devel)\n",
    "    \n",
    "test_for_del = dev_corpus[:1000]\n",
    "\n",
    "test_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/testing.json\",'r') as test:\n",
    "    test_corpus = json.load(test)\n",
    "    \n",
    "test_data = test_corpus[:10]\n",
    "\n",
    "punc = string.punctuation\n",
    "stopwordsAll = set(stopwords.words('english'))\n",
    "    \n",
    "def qestion_and_answer(question_id):\n",
    "    question = train_corpus[question_id]['question']\n",
    "    answer = train_corpus[question_id]['text']\n",
    "    processed_question = word_tokenize(question)\n",
    "    processed_answer = word_tokenize(answer)\n",
    "    para_id = train_corpus[question_id]['answer_paragraph']\n",
    "    docid = train_corpus[question_id]['docid']\n",
    "    #print(\"processed_question\",processed_question)\n",
    "    #print(\"answer\",answer)\n",
    "    #print(\"para_id\",para_id)\n",
    "    return processed_question, processed_answer, para_id, docid\n",
    "\n",
    "#qestion_and_answer(0)  \n",
    "    \n",
    "def doc_to_story(para_id, docid):\n",
    "    story = []\n",
    "    doc = docs_corpus[docid]\n",
    "    #print(\"doc\",doc)\n",
    "    para = \" \"\n",
    "    for index, para_data in enumerate(doc['text']):\n",
    "        if index == para_id:\n",
    "            para = para_data\n",
    "    #print(\"para\",para)\n",
    "    sents = sent_tokenize(para)\n",
    "    #print(\"sents\",sents)\n",
    "    for sent in sents:\n",
    "        tokens = word_tokenize(sent)\n",
    "        #print(\"tokens\",tokens)\n",
    "        story.append(tokens)\n",
    "    #print(\"story\",story)\n",
    "    return story\n",
    "            \n",
    "#doc_to_story(0)\n",
    "\n",
    "\n",
    "\n",
    "#save [(story, question, answer)]\n",
    "def prepare_data(train_corpus):\n",
    "    final_data = []\n",
    "    print(len(train_corpus))\n",
    "    for i in range(len(train_corpus)):\n",
    "        processed_question, answer, para_id, docid = qestion_and_answer(i)\n",
    "        story = doc_to_story(para_id, docid)\n",
    "        final_data.append((story, processed_question, answer))\n",
    "        #print(\"final_data\",final_data)\n",
    "    return final_data\n",
    "  \n",
    "data = prepare_data(test_for_train)\n",
    "print(\"len(data)\", len(data))\n",
    "'''\n",
    "del_data = prepare_data(test_for_del)\n",
    "print(\"len(del_data)\",len(del_data))\n",
    "'''\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        story_list = []\n",
    "        for sent in story:\n",
    "            for w in sent:\n",
    "                story_list.append(w)\n",
    "        x = [word_idx[w]  for w in story_list]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        for token in answer:\n",
    "            y[word_idx[token]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)\n",
    "\n",
    "\n",
    "def get_paragraph(docid,document_data):\n",
    "    #get the paragraph that contains the answer\n",
    "    for item in document_data:\n",
    "        if item['docid'] == docid:\n",
    "            document = item['text']\n",
    "            break\n",
    "    return document\n",
    "\n",
    "def term_freqs(document):\n",
    "    tfs = defaultdict(dict)\n",
    "    tfs_forward = defaultdict(dict)\n",
    "    doc_id = 0\n",
    "    for sentence in document:\n",
    "        for token in word_tokenize(sentence):\n",
    "            if token not in stopwordsAll and token not in punc:  \n",
    "                term = lemmatize(token.lower())\n",
    "                tfs[term][doc_id] = tfs[term].get(doc_id, 0) + 1 \n",
    "                tfs_forward[doc_id][term] = tfs[doc_id].get(term, 0) + 1 \n",
    "        doc_id += 1\n",
    "    return tfs,doc_id+1,tfs_forward\n",
    "\n",
    "def get_okapibm25(tf, total_docment, documents):\n",
    "    '''Calculate and return term weights based on okapibm25'''\n",
    "    k1, b, k3 = 1.5, 0.5, 0\n",
    "    okapibm25 = defaultdict(dict)\n",
    "\n",
    "    # calculate average doc length \n",
    "    total = 0\n",
    "    for d in documents:\n",
    "        total += len(d)\n",
    "    avg_doc_length = total/len(documents)*1.0\n",
    "\n",
    "    for term, doc_list in tf.items():\n",
    "        df = len(doc_list)\n",
    "        for doc_id, freq in doc_list.items():\n",
    "            # term occurences in query\n",
    "            # qtf = question.count(term) # SEPCIAL \n",
    "            qtf = 1.2\n",
    "            idf = log((total_docment-df+0.5) / df+0.5)\n",
    "            tf_Dt = ((k1+1)*tf[term][doc_id]) / (k1*((1-b)+b*(len(documents[doc_id])/avg_doc_length) + tf[term][doc_id]))\n",
    "            if qtf == 0:\n",
    "                third = 0\n",
    "            else:\n",
    "                third = ((k3+1)*qtf) / (k3+qtf)\n",
    "                okapibm25[term][doc_id] = idf*tf_Dt*third\n",
    "\n",
    "    return okapibm25\n",
    "\n",
    "#find top_k paragraph that may contain the answer\n",
    "def get_top_k_document(tfidf,query,k,document):\n",
    "    top_document_id = Counter()\n",
    "    for token in word_tokenize(query):\n",
    "        if token not in stopwordsAll:  \n",
    "             term = lemmatizer.lemmatize(token.lower())\n",
    "             term_tfidf = tfidf[term]\n",
    "             for docid, weight in term_tfidf.items():\n",
    "                 top_document_id[docid] += weight\n",
    "    top_document_id = top_document_id.most_common(k)\n",
    "    top_document = []\n",
    "    for document_id,weight in top_document_id:\n",
    "        top_document.append(document_id)\n",
    "    return top_document\n",
    "\n",
    "def prepare_test_data(test_corpus):\n",
    "    final_data = []\n",
    "    for item in test_data:\n",
    "        question = item['question']\n",
    "        docid = item['docid']\n",
    "        processed_question = word_tokenize(question)\n",
    "        document = get_paragraph(docid,docs_corpus)\n",
    "        tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "        tfidf = get_okapibm25(tfs, total_docment,document)\n",
    "        top_1 = get_top_k_document(tfidf,question,1,document)\n",
    "        for item in top_1:\n",
    "            print(\"top_1\", top_1)\n",
    "            story = doc_to_story(top_1, docid)\n",
    "            print(\"story\",story)\n",
    "        final_data.append((story, processed_question, answer))\n",
    "        #print(\"final_data\",final_data)\n",
    "    return \n",
    "\n",
    "def prepare_test_del(train_corpus):\n",
    "    final_data = []\n",
    "    print(len(train_corpus))\n",
    "    for i in range(len(train_corpus)):\n",
    "        story = []\n",
    "        question = train_corpus[i]['question']\n",
    "        processed_question, answer, para_id, docid = qestion_and_answer(i)\n",
    "        document = get_paragraph(docid,docs_corpus)\n",
    "        tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "        tfidf = get_okapibm25(tfs, total_docment,document)\n",
    "        top_1 = get_top_k_document(tfidf,question,1,document)\n",
    "        for item in top_1:\n",
    "            #print(\"item\", item)\n",
    "            story = doc_to_story(item, docid)\n",
    "            #print(\"story\",story)\n",
    "        final_data.append((story, processed_question, answer))\n",
    "        #print(\"final_data\",final_data)\n",
    "    return final_data\n",
    "\n",
    "del_data = prepare_test_del(test_for_del)\n",
    "print(\"len(del_data)\",len(del_data))\n",
    "\n",
    "def get_vocab(data):\n",
    "    vocab = set()\n",
    "    for story, q, answer in data:\n",
    "        list_words = []\n",
    "        for sent in story:            \n",
    "            list_words = list_words + sent \n",
    "        #print(\"list_words\",list_words)\n",
    "        #print(\"q\",q)\n",
    "        #print(\"answer\",answer)\n",
    "        vocab_list = list_words + q + answer\n",
    "        #print(\"vocab_list\",vocab_list)\n",
    "        vocab |= set(vocab_list)\n",
    "    vocab = sorted(vocab)\n",
    "    return vocab\n",
    "vocab = get_vocab(data + del_data)\n",
    "#print(\"vocab\",vocab)\n",
    "print(\"len(vocab)\",len(vocab))\n",
    "\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word2idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in data + del_data)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in data + del_data)))\n",
    "\n",
    "x, xq, y = vectorize_stories(data, word2idx, story_maxlen, query_maxlen)\n",
    "tx, txq, ty = vectorize_stories(del_data, word2idx, story_maxlen, query_maxlen)\n",
    "\n",
    "modelname = \"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/mymodel.bin.gz\"\n",
    "model = Word2Vec.load(modelname)\n",
    "print(\"Loading Word2Vec model and generating embedding matrix...\")\n",
    "#word2vec = gensim.models.KeyedVectors.load_word2vec_format(modelname\n",
    "#    , binary=True)\n",
    "print(model.wv['computer'])\n",
    "embedding_weights = np.zeros((vocab_size, WORD2VEC_EMBED_SIZE))\n",
    "for word, index in word2idx.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = model.wv[word.lower()]\n",
    "    except KeyError:\n",
    "        pass  # keep as zero (not ideal, but what else can we do?)\n",
    "\n",
    "#print('vocab = {}'.format(vocab))\n",
    "print('x.shape = {}'.format(x.shape))\n",
    "print('xq.shape = {}'.format(xq.shape))\n",
    "print('y.shape = {}'.format(y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "\n",
    "sentence = layers.Input(shape=(story_maxlen,), dtype='int32')\n",
    "encoded_sentence = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE,weights=[embedding_weights])(sentence)\n",
    "encoded_sentence = layers.Dropout(0.3)(encoded_sentence)\n",
    "encoded_question = RNN(EMBED_HIDDEN_SIZE)(encoded_sentence)\n",
    "\n",
    "question = layers.Input(shape=(query_maxlen,), dtype='int32')\n",
    "encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE,weights=[embedding_weights])(question)\n",
    "encoded_question = layers.Dropout(0.3)(encoded_question)\n",
    "encoded_question = RNN(EMBED_HIDDEN_SIZE)(encoded_question)\n",
    "encoded_question = layers.RepeatVector(story_maxlen)(encoded_question)\n",
    "\n",
    "merged = layers.add([encoded_sentence, encoded_question])\n",
    "merged = RNN(EMBED_HIDDEN_SIZE)(merged)\n",
    "merged = layers.Dropout(0.3)(merged)\n",
    "preds = layers.Dense(vocab_size, activation='softmax')(merged)\n",
    "\n",
    "model = Model([sentence, question], preds)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training')\n",
    "model.fit([x, xq], y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.05)\n",
    "loss, acc = model.evaluate([tx, txq], ty,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
