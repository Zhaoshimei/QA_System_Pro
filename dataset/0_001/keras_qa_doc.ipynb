{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimei/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_corpus) 43379\n",
      "10000\n",
      "len(data) 10000\n",
      "100\n",
      "len(del_data) 100\n",
      "len(vocab) 60415\n",
      "RNN / Embed / Sent / Query = <class 'keras.layers.recurrent.LSTM'>, 50, 100, 100\n",
      "x.shape = (10000, 581)\n",
      "xq.shape = (10000, 60)\n",
      "y.shape = (10000, 60416)\n",
      "story_maxlen, query_maxlen = 581, 60\n",
      "Build model...\n",
      "Training\n",
      "Train on 9500 samples, validate on 500 samples\n",
      "Epoch 1/40\n",
      "9500/9500 [==============================] - 217s 23ms/step - loss: 18.1150 - acc: 0.0021 - val_loss: 17.9859 - val_acc: 0.0020\n",
      "Epoch 2/40\n",
      "9500/9500 [==============================] - 211s 22ms/step - loss: 16.4165 - acc: 0.0019 - val_loss: 18.4914 - val_acc: 0.0020\n",
      "Epoch 3/40\n",
      "9500/9500 [==============================] - 212s 22ms/step - loss: 16.0382 - acc: 0.0019 - val_loss: 18.8200 - val_acc: 0.0020\n",
      "Epoch 4/40\n",
      "9500/9500 [==============================] - 231s 24ms/step - loss: 15.7885 - acc: 0.0019 - val_loss: 19.1268 - val_acc: 0.0020\n",
      "Epoch 5/40\n",
      "9500/9500 [==============================] - 216s 23ms/step - loss: 15.6422 - acc: 0.0019 - val_loss: 19.3043 - val_acc: 0.0020\n",
      "Epoch 6/40\n",
      "9500/9500 [==============================] - 217s 23ms/step - loss: 15.5410 - acc: 0.0019 - val_loss: 19.2897 - val_acc: 0.0020\n",
      "Epoch 7/40\n",
      "9500/9500 [==============================] - 215s 23ms/step - loss: 15.5092 - acc: 0.0020 - val_loss: 19.2828 - val_acc: 0.0020\n",
      "Epoch 8/40\n",
      "9500/9500 [==============================] - 211s 22ms/step - loss: 15.5270 - acc: 0.0019 - val_loss: 19.2914 - val_acc: 0.0020\n",
      "Epoch 9/40\n",
      "9500/9500 [==============================] - 213s 22ms/step - loss: 15.4949 - acc: 0.0019 - val_loss: 19.2987 - val_acc: 0.0020\n",
      "Epoch 10/40\n",
      "9500/9500 [==============================] - 221s 23ms/step - loss: 15.4859 - acc: 0.0019 - val_loss: 19.2977 - val_acc: 0.0020\n",
      "Epoch 11/40\n",
      "9500/9500 [==============================] - 219s 23ms/step - loss: 15.4881 - acc: 0.0019 - val_loss: 19.2951 - val_acc: 0.0020\n",
      "Epoch 12/40\n",
      "9500/9500 [==============================] - 217s 23ms/step - loss: 15.4727 - acc: 0.0019 - val_loss: 19.2924 - val_acc: 0.0020\n",
      "Epoch 13/40\n",
      "9500/9500 [==============================] - 227s 24ms/step - loss: 15.4809 - acc: 0.0019 - val_loss: 19.2924 - val_acc: 0.0020\n",
      "Epoch 14/40\n",
      "9500/9500 [==============================] - 220s 23ms/step - loss: 15.5010 - acc: 0.0019 - val_loss: 19.2895 - val_acc: 0.0020\n",
      "Epoch 15/40\n",
      "9500/9500 [==============================] - 219s 23ms/step - loss: 15.4648 - acc: 0.0019 - val_loss: 19.3058 - val_acc: 0.0020\n",
      "Epoch 16/40\n",
      "9500/9500 [==============================] - 219s 23ms/step - loss: 15.4800 - acc: 0.0019 - val_loss: 19.2837 - val_acc: 0.0020\n",
      "Epoch 17/40\n",
      "9500/9500 [==============================] - 216s 23ms/step - loss: 15.4779 - acc: 0.0019 - val_loss: 19.2863 - val_acc: 0.0020\n",
      "Epoch 18/40\n",
      "9500/9500 [==============================] - 217s 23ms/step - loss: 15.4593 - acc: 0.0019 - val_loss: 19.2878 - val_acc: 0.0020\n",
      "Epoch 19/40\n",
      "9500/9500 [==============================] - 214s 22ms/step - loss: 15.4589 - acc: 0.0019 - val_loss: 19.3024 - val_acc: 0.0020\n",
      "Epoch 20/40\n",
      "9500/9500 [==============================] - 212s 22ms/step - loss: 15.4544 - acc: 0.0019 - val_loss: 19.3012 - val_acc: 0.0020\n",
      "Epoch 21/40\n",
      "9500/9500 [==============================] - 218s 23ms/step - loss: 15.4642 - acc: 0.0019 - val_loss: 19.2724 - val_acc: 0.0020\n",
      "Epoch 22/40\n",
      "9500/9500 [==============================] - 218s 23ms/step - loss: 15.4410 - acc: 0.0019 - val_loss: 19.2949 - val_acc: 0.0020\n",
      "Epoch 23/40\n",
      "9500/9500 [==============================] - 226s 24ms/step - loss: 15.4591 - acc: 0.0019 - val_loss: 19.3026 - val_acc: 0.0020\n",
      "Epoch 24/40\n",
      "9500/9500 [==============================] - 214s 23ms/step - loss: 15.4417 - acc: 0.0019 - val_loss: 19.2846 - val_acc: 0.0020\n",
      "Epoch 25/40\n",
      "9500/9500 [==============================] - 219s 23ms/step - loss: 15.4523 - acc: 0.0019 - val_loss: 19.2907 - val_acc: 0.0020\n",
      "Epoch 26/40\n",
      "9500/9500 [==============================] - 226s 24ms/step - loss: 15.4517 - acc: 0.0019 - val_loss: 19.3015 - val_acc: 0.0020\n",
      "Epoch 27/40\n",
      "9500/9500 [==============================] - 225s 24ms/step - loss: 15.4429 - acc: 0.0019 - val_loss: 19.2975 - val_acc: 0.0020\n",
      "Epoch 28/40\n",
      "9500/9500 [==============================] - 222s 23ms/step - loss: 15.4465 - acc: 0.0019 - val_loss: 19.2984 - val_acc: 0.0020\n",
      "Epoch 29/40\n",
      "9500/9500 [==============================] - 218s 23ms/step - loss: 15.4209 - acc: 0.0019 - val_loss: 19.2890 - val_acc: 0.0020\n",
      "Epoch 30/40\n",
      "9500/9500 [==============================] - 217s 23ms/step - loss: 15.4547 - acc: 0.0019 - val_loss: 19.2793 - val_acc: 0.0020\n",
      "Epoch 31/40\n",
      "9500/9500 [==============================] - 226s 24ms/step - loss: 15.4294 - acc: 0.0019 - val_loss: 19.2812 - val_acc: 0.0020\n",
      "Epoch 32/40\n",
      "9500/9500 [==============================] - 211s 22ms/step - loss: 15.4279 - acc: 0.0019 - val_loss: 19.2967 - val_acc: 0.0020\n",
      "Epoch 33/40\n",
      "9500/9500 [==============================] - 225s 24ms/step - loss: 15.4390 - acc: 0.0019 - val_loss: 19.2753 - val_acc: 0.0020\n",
      "Epoch 34/40\n",
      "9500/9500 [==============================] - 217s 23ms/step - loss: 15.4213 - acc: 0.0019 - val_loss: 19.3033 - val_acc: 0.0020\n",
      "Epoch 35/40\n",
      "9500/9500 [==============================] - 231s 24ms/step - loss: 15.4122 - acc: 0.0019 - val_loss: 19.2866 - val_acc: 0.0020\n",
      "Epoch 36/40\n",
      "9500/9500 [==============================] - 218s 23ms/step - loss: 15.4225 - acc: 0.0019 - val_loss: 19.2842 - val_acc: 0.0020\n",
      "Epoch 37/40\n",
      "9500/9500 [==============================] - 213s 22ms/step - loss: 15.4203 - acc: 0.0019 - val_loss: 19.3018 - val_acc: 0.0020\n",
      "Epoch 38/40\n",
      "9500/9500 [==============================] - 214s 23ms/step - loss: 15.4351 - acc: 0.0019 - val_loss: 19.2896 - val_acc: 0.0020\n",
      "Epoch 39/40\n",
      "9500/9500 [==============================] - 208s 22ms/step - loss: 15.4238 - acc: 0.0019 - val_loss: 19.2964 - val_acc: 0.0020\n",
      "Epoch 40/40\n",
      "9500/9500 [==============================] - 221s 23ms/step - loss: 15.4244 - acc: 0.0019 - val_loss: 19.2950 - val_acc: 0.0020\n",
      "100/100 [==============================] - 1s 7ms/step\n",
      "Test loss / test accuracy = 14.8538 / 0.0000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "docs_corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/documents.json\",'r') as docs:\n",
    "    docs_corpus = json.load(docs)\n",
    "            \n",
    "train_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/training.json\",'r') as training:\n",
    "    train_corpus = json.load(training)\n",
    "print(\"len(train_corpus)\",len(train_corpus))\n",
    "\n",
    "test_for_train = train_corpus[:10000]\n",
    "\n",
    "dev_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/devel.json\",'r') as devel:\n",
    "    dev_corpus = json.load(devel)\n",
    "    \n",
    "test_for_del = dev_corpus[:100]\n",
    "\n",
    "    \n",
    "def qestion_and_answer(question_id):\n",
    "    question = train_corpus[question_id]['question']\n",
    "    answer = train_corpus[question_id]['text']\n",
    "    processed_question = word_tokenize(question)\n",
    "    para_id = train_corpus[question_id]['answer_paragraph']\n",
    "    #print(\"processed_question\",processed_question)\n",
    "    #print(\"answer\",answer)\n",
    "    #print(\"para_id\",para_id)\n",
    "    return processed_question, answer, para_id\n",
    "\n",
    "#qestion_and_answer(0)  \n",
    "    \n",
    "def doc_to_story(para_id):\n",
    "    story = []\n",
    "    i = -1\n",
    "    doc = docs_corpus[para_id]\n",
    "    for para in doc['text']:\n",
    "        sents = sent_tokenize(para)\n",
    "        #print(\"sents\",sents)\n",
    "        for sent in sents:\n",
    "            i +=1\n",
    "            tokens = word_tokenize(sent)\n",
    "            #print(\"tokens\",tokens)\n",
    "            story.append(tokens)\n",
    "    #print(\"story\",story)\n",
    "    return story\n",
    "            \n",
    "#doc_to_story(0)\n",
    "\n",
    "#save [(story, question, answer)]\n",
    "def prepare_data(train_corpus):\n",
    "    final_data = []\n",
    "    print(len(train_corpus))\n",
    "    for i in range(len(train_corpus)):\n",
    "        processed_question, answer, docid = qestion_and_answer(i)\n",
    "        story = doc_to_story(docid)\n",
    "        final_data.append((story, processed_question, answer))\n",
    "        #print(\"final_data\",final_data)\n",
    "    return final_data\n",
    "    \n",
    "data = prepare_data(test_for_train)\n",
    "print(\"len(data)\", len(data))\n",
    "\n",
    "del_data = prepare_data(test_for_del)\n",
    "print(\"len(del_data)\",len(del_data))\n",
    "\n",
    "'''\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "'''\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        story_list = []\n",
    "        for sent in story:\n",
    "            for w in sent:\n",
    "                story_list.append(w)\n",
    "        x = [word_idx[w]  for w in story_list]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        for token in word_tokenize(answer):\n",
    "            y[word_idx[token]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)\n",
    "\n",
    "\n",
    "def get_vocab(data):\n",
    "    vocab = set()\n",
    "    for story, q, answer in data:\n",
    "        list_words = []\n",
    "        for sent in story:            \n",
    "            list_words = list_words + sent \n",
    "        vocab_list = list_words + q + word_tokenize(answer)\n",
    "        #print(\"vocab_list\",vocab_list)\n",
    "        vocab |= set(vocab_list)\n",
    "    vocab = sorted(vocab)\n",
    "    return vocab\n",
    "vocab = get_vocab(data)\n",
    "#print(\"vocab\",vocab)\n",
    "print(\"len(vocab)\",len(vocab))\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in data)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in data)))\n",
    "\n",
    "x, xq, y = vectorize_stories(data, word_idx, story_maxlen, query_maxlen)\n",
    "tx, txq, ty = vectorize_stories(del_data, word_idx, story_maxlen, query_maxlen)\n",
    "\n",
    "#print('vocab = {}'.format(vocab))\n",
    "print('x.shape = {}'.format(x.shape))\n",
    "print('xq.shape = {}'.format(xq.shape))\n",
    "print('y.shape = {}'.format(y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "\n",
    "sentence = layers.Input(shape=(story_maxlen,), dtype='int32')\n",
    "encoded_sentence = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence)\n",
    "encoded_sentence = layers.Dropout(0.3)(encoded_sentence)\n",
    "\n",
    "question = layers.Input(shape=(query_maxlen,), dtype='int32')\n",
    "encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\n",
    "encoded_question = layers.Dropout(0.3)(encoded_question)\n",
    "encoded_question = RNN(EMBED_HIDDEN_SIZE)(encoded_question)\n",
    "encoded_question = layers.RepeatVector(story_maxlen)(encoded_question)\n",
    "\n",
    "merged = layers.add([encoded_sentence, encoded_question])\n",
    "merged = RNN(EMBED_HIDDEN_SIZE)(merged)\n",
    "merged = layers.Dropout(0.3)(merged)\n",
    "preds = layers.Dense(vocab_size, activation='softmax')(merged)\n",
    "\n",
    "model = Model([sentence, question], preds)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training')\n",
    "model.fit([x, xq], y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.05)\n",
    "loss, acc = model.evaluate([tx, txq], ty,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
