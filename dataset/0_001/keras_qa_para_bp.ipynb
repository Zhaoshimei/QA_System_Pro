{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_corpus) 43379\n",
      "10000\n",
      "len(data) 10000\n",
      "1000\n",
      "len(del_data) 1000\n",
      "len(vocab) 41855\n",
      "RNN / Embed / Sent / Query = <class 'keras.layers.recurrent.LSTM'>, 50, 100, 100\n",
      "x.shape = (10000, 21)\n",
      "xq.shape = (10000, 60)\n",
      "y.shape = (10000, 41856)\n",
      "story_maxlen, query_maxlen = 21, 60\n",
      "Build model...\n",
      "Training\n",
      "Train on 9500 samples, validate on 500 samples\n",
      "Epoch 1/40\n",
      "9500/9500 [==============================] - 137s 14ms/step - loss: 17.7488 - acc: 0.0022 - val_loss: 17.6712 - val_acc: 0.0020\n",
      "Epoch 2/40\n",
      "9500/9500 [==============================] - 136s 14ms/step - loss: 16.2673 - acc: 0.0019 - val_loss: 18.2030 - val_acc: 0.0020\n",
      "Epoch 3/40\n",
      "9500/9500 [==============================] - 134s 14ms/step - loss: 15.9270 - acc: 0.0019 - val_loss: 18.6145 - val_acc: 0.0020\n",
      "Epoch 4/40\n",
      "9500/9500 [==============================] - 133s 14ms/step - loss: 15.7171 - acc: 0.0019 - val_loss: 18.9307 - val_acc: 0.0020\n",
      "Epoch 5/40\n",
      "9500/9500 [==============================] - 141s 15ms/step - loss: 15.5841 - acc: 0.0019 - val_loss: 19.2138 - val_acc: 0.0020\n",
      "Epoch 6/40\n",
      "9500/9500 [==============================] - 131s 14ms/step - loss: 15.5157 - acc: 0.0019 - val_loss: 19.2919 - val_acc: 0.0020\n",
      "Epoch 7/40\n",
      "9500/9500 [==============================] - 129s 14ms/step - loss: 15.4825 - acc: 0.0019 - val_loss: 19.2830 - val_acc: 0.0020\n",
      "Epoch 8/40\n",
      "9500/9500 [==============================] - 138s 14ms/step - loss: 15.4749 - acc: 0.0019 - val_loss: 19.2857 - val_acc: 0.0020\n",
      "Epoch 9/40\n",
      "9500/9500 [==============================] - 127s 13ms/step - loss: 15.4630 - acc: 0.0019 - val_loss: 19.2957 - val_acc: 0.0020\n",
      "Epoch 10/40\n",
      "9500/9500 [==============================] - 125s 13ms/step - loss: 15.4718 - acc: 0.0019 - val_loss: 19.3019 - val_acc: 0.0020\n",
      "Epoch 11/40\n",
      "9500/9500 [==============================] - 128s 13ms/step - loss: 15.4559 - acc: 0.0019 - val_loss: 19.2874 - val_acc: 0.0020\n",
      "Epoch 12/40\n",
      "9500/9500 [==============================] - 139s 15ms/step - loss: 15.4597 - acc: 0.0019 - val_loss: 19.3018 - val_acc: 0.0020\n",
      "Epoch 13/40\n",
      "9500/9500 [==============================] - 133s 14ms/step - loss: 15.4543 - acc: 0.0019 - val_loss: 19.2736 - val_acc: 0.0020\n",
      "Epoch 14/40\n",
      "9500/9500 [==============================] - 138s 15ms/step - loss: 15.4457 - acc: 0.0019 - val_loss: 19.2930 - val_acc: 0.0020\n",
      "Epoch 15/40\n",
      "9500/9500 [==============================] - 131s 14ms/step - loss: 15.4499 - acc: 0.0019 - val_loss: 19.2986 - val_acc: 0.0020\n",
      "Epoch 16/40\n",
      "9500/9500 [==============================] - 130s 14ms/step - loss: 15.4428 - acc: 0.0019 - val_loss: 19.2835 - val_acc: 0.0020\n",
      "Epoch 17/40\n",
      "9500/9500 [==============================] - 127s 13ms/step - loss: 15.4465 - acc: 0.0019 - val_loss: 19.2885 - val_acc: 0.0020\n",
      "Epoch 18/40\n",
      "9500/9500 [==============================] - 127s 13ms/step - loss: 15.4465 - acc: 0.0019 - val_loss: 19.2825 - val_acc: 0.0020\n",
      "Epoch 19/40\n",
      "9500/9500 [==============================] - 127s 13ms/step - loss: 15.4558 - acc: 0.0019 - val_loss: 19.2936 - val_acc: 0.0020\n",
      "Epoch 20/40\n",
      "9500/9500 [==============================] - 128s 13ms/step - loss: 15.4561 - acc: 0.0019 - val_loss: 19.2894 - val_acc: 0.0020\n",
      "Epoch 21/40\n",
      "9500/9500 [==============================] - 128s 13ms/step - loss: 15.4571 - acc: 0.0019 - val_loss: 19.3021 - val_acc: 0.0020\n",
      "Epoch 22/40\n",
      "9500/9500 [==============================] - 127s 13ms/step - loss: 15.4498 - acc: 0.0019 - val_loss: 19.2806 - val_acc: 0.0020\n",
      "Epoch 23/40\n",
      "9500/9500 [==============================] - 114s 12ms/step - loss: 15.4417 - acc: 0.0019 - val_loss: 19.2920 - val_acc: 0.0020\n",
      "Epoch 24/40\n",
      "9500/9500 [==============================] - 72s 8ms/step - loss: 15.4319 - acc: 0.0019 - val_loss: 19.2944 - val_acc: 0.0020\n",
      "Epoch 25/40\n",
      "9500/9500 [==============================] - 69s 7ms/step - loss: 15.4323 - acc: 0.0019 - val_loss: 19.2902 - val_acc: 0.0020\n",
      "Epoch 26/40\n",
      "9500/9500 [==============================] - 69s 7ms/step - loss: 15.4312 - acc: 0.0019 - val_loss: 19.3162 - val_acc: 0.0020\n",
      "Epoch 27/40\n",
      "9500/9500 [==============================] - 70s 7ms/step - loss: 15.4251 - acc: 0.0019 - val_loss: 19.3102 - val_acc: 0.0020\n",
      "Epoch 28/40\n",
      "9500/9500 [==============================] - 70s 7ms/step - loss: 15.4405 - acc: 0.0019 - val_loss: 19.3093 - val_acc: 0.0020\n",
      "Epoch 29/40\n",
      "9500/9500 [==============================] - 70s 7ms/step - loss: 15.4234 - acc: 0.0019 - val_loss: 19.2843 - val_acc: 0.0020\n",
      "Epoch 30/40\n",
      "9500/9500 [==============================] - 69s 7ms/step - loss: 15.4181 - acc: 0.0019 - val_loss: 19.2968 - val_acc: 0.0020\n",
      "Epoch 31/40\n",
      "9500/9500 [==============================] - 69s 7ms/step - loss: 15.4214 - acc: 0.0019 - val_loss: 19.3030 - val_acc: 0.0020\n",
      "Epoch 32/40\n",
      "9500/9500 [==============================] - 69s 7ms/step - loss: 15.4020 - acc: 0.0019 - val_loss: 19.3021 - val_acc: 0.0020\n",
      "Epoch 33/40\n",
      "9500/9500 [==============================] - 69s 7ms/step - loss: 15.3903 - acc: 0.0019 - val_loss: 19.2955 - val_acc: 0.0020\n",
      "Epoch 34/40\n",
      "9500/9500 [==============================] - 69s 7ms/step - loss: 15.4191 - acc: 0.0019 - val_loss: 19.2907 - val_acc: 0.0020\n",
      "Epoch 35/40\n",
      "9500/9500 [==============================] - 69s 7ms/step - loss: 15.4089 - acc: 0.0019 - val_loss: 19.2853 - val_acc: 0.0020\n",
      "Epoch 36/40\n",
      "9500/9500 [==============================] - 69s 7ms/step - loss: 15.4043 - acc: 0.0019 - val_loss: 19.2794 - val_acc: 0.0020\n",
      "Epoch 37/40\n",
      "9500/9500 [==============================] - 69s 7ms/step - loss: 15.4159 - acc: 0.0019 - val_loss: 19.2885 - val_acc: 0.0020\n",
      "Epoch 38/40\n",
      "9500/9500 [==============================] - 69s 7ms/step - loss: 15.4216 - acc: 0.0019 - val_loss: 19.2859 - val_acc: 0.0020\n",
      "Epoch 39/40\n",
      "9500/9500 [==============================] - 69s 7ms/step - loss: 15.4062 - acc: 0.0019 - val_loss: 19.2819 - val_acc: 0.0020\n",
      "Epoch 40/40\n",
      "9500/9500 [==============================] - 69s 7ms/step - loss: 15.4031 - acc: 0.0019 - val_loss: 19.2932 - val_acc: 0.0020\n",
      "1000/1000 [==============================] - 2s 2ms/step\n",
      "Test loss / test accuracy = 15.9259 / 0.0020\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from math import log\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "docs_corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/documents.json\",'r') as docs:\n",
    "    docs_corpus = json.load(docs)\n",
    "            \n",
    "train_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/training.json\",'r') as training:\n",
    "    train_corpus = json.load(training)\n",
    "print(\"len(train_corpus)\",len(train_corpus))\n",
    "\n",
    "test_for_train = train_corpus[:10000]\n",
    "\n",
    "dev_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/devel.json\",'r') as devel:\n",
    "    dev_corpus = json.load(devel)\n",
    "    \n",
    "test_for_del = dev_corpus[:1000]\n",
    "\n",
    "test_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/testing.json\",'r') as test:\n",
    "    test_corpus = json.load(test)\n",
    "    \n",
    "#test_data = test_corpus[:10]\n",
    "\n",
    "punc = string.punctuation\n",
    "stopwordsAll = set(stopwords.words('english'))\n",
    "    \n",
    "def qestion_and_answer(question_id):\n",
    "    question = train_corpus[question_id]['question']\n",
    "    answer = train_corpus[question_id]['text']\n",
    "    processed_question = word_tokenize(question)\n",
    "    para_id = train_corpus[question_id]['answer_paragraph']\n",
    "    docid = train_corpus[question_id]['docid']\n",
    "    #print(\"processed_question\",processed_question)\n",
    "    #print(\"answer\",answer)\n",
    "    #print(\"para_id\",para_id)\n",
    "    return processed_question, answer, para_id, docid\n",
    "\n",
    "#qestion_and_answer(0)  \n",
    "    \n",
    "def doc_to_story(para_id, docid):\n",
    "    story = []\n",
    "    doc = docs_corpus[docid]\n",
    "    #print(\"doc\",doc)\n",
    "    para = \" \"\n",
    "    for index, para_data in enumerate(doc['text']):\n",
    "        if index == para_id:\n",
    "            para = para_data\n",
    "    #print(\"para\",para)\n",
    "    sents = sent_tokenize(para)\n",
    "    #print(\"sents\",sents)\n",
    "    for sent in sents:\n",
    "        tokens = word_tokenize(sent)\n",
    "        #print(\"tokens\",tokens)\n",
    "        story.append(tokens)\n",
    "    #print(\"story\",story)\n",
    "    return story\n",
    "            \n",
    "#doc_to_story(0)\n",
    "\n",
    "#save [(story, question, answer)]\n",
    "def prepare_data(train_corpus):\n",
    "    final_data = []\n",
    "    print(len(train_corpus))\n",
    "    for i in range(len(train_corpus)):\n",
    "        processed_question, answer, para_id, docid = qestion_and_answer(i)\n",
    "        story = doc_to_story(para_id, docid)\n",
    "        final_data.append((story, processed_question, answer))\n",
    "        #print(\"final_data\",final_data)\n",
    "    return final_data\n",
    "  \n",
    "data = prepare_data(test_for_train)\n",
    "print(\"len(data)\", len(data))\n",
    "'''\n",
    "del_data = prepare_data(test_for_del)\n",
    "print(\"len(del_data)\",len(del_data))\n",
    "'''\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        story_list = []\n",
    "        for sent in story:\n",
    "            for w in sent:\n",
    "                story_list.append(w)\n",
    "        x = [word_idx[w]  for w in story_list]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        for token in word_tokenize(answer):\n",
    "            y[word_idx[token]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)\n",
    "\n",
    "\n",
    "def get_paragraph(docid,document_data):\n",
    "    #get the paragraph that contains the answer\n",
    "    for item in document_data:\n",
    "        if item['docid'] == docid:\n",
    "            document = item['text']\n",
    "            break\n",
    "    return document\n",
    "\n",
    "def term_freqs(document):\n",
    "    tfs = defaultdict(dict)\n",
    "    tfs_forward = defaultdict(dict)\n",
    "    doc_id = 0\n",
    "    for sentence in document:\n",
    "        for token in word_tokenize(sentence):\n",
    "            if token not in stopwordsAll and token not in punc:  \n",
    "                term = lemmatize(token.lower())\n",
    "                tfs[term][doc_id] = tfs[term].get(doc_id, 0) + 1 \n",
    "                tfs_forward[doc_id][term] = tfs[doc_id].get(term, 0) + 1 \n",
    "        doc_id += 1\n",
    "    return tfs,doc_id+1,tfs_forward\n",
    "\n",
    "def get_okapibm25(tf, total_docment, documents):\n",
    "    '''Calculate and return term weights based on okapibm25'''\n",
    "    k1, b, k3 = 1.5, 0.5, 0\n",
    "    okapibm25 = defaultdict(dict)\n",
    "\n",
    "    # calculate average doc length \n",
    "    total = 0\n",
    "    for d in documents:\n",
    "        total += len(d)\n",
    "    avg_doc_length = total/len(documents)*1.0\n",
    "\n",
    "    for term, doc_list in tf.items():\n",
    "        df = len(doc_list)\n",
    "        for doc_id, freq in doc_list.items():\n",
    "            # term occurences in query\n",
    "            # qtf = question.count(term) # SEPCIAL \n",
    "            qtf = 1.2\n",
    "            idf = log((total_docment-df+0.5) / df+0.5)\n",
    "            tf_Dt = ((k1+1)*tf[term][doc_id]) / (k1*((1-b)+b*(len(documents[doc_id])/avg_doc_length) + tf[term][doc_id]))\n",
    "            if qtf == 0:\n",
    "                third = 0\n",
    "            else:\n",
    "                third = ((k3+1)*qtf) / (k3+qtf)\n",
    "                okapibm25[term][doc_id] = idf*tf_Dt*third\n",
    "\n",
    "    return okapibm25\n",
    "\n",
    "#find top_k paragraph that may contain the answer\n",
    "def get_top_k_document(tfidf,query,k,document):\n",
    "    top_document_id = Counter()\n",
    "    for token in word_tokenize(query):\n",
    "        if token not in stopwordsAll:  \n",
    "             term = lemmatizer.lemmatize(token.lower())\n",
    "             term_tfidf = tfidf[term]\n",
    "             for docid, weight in term_tfidf.items():\n",
    "                 top_document_id[docid] += weight\n",
    "    top_document_id = top_document_id.most_common(k)\n",
    "    top_document = []\n",
    "    for document_id,weight in top_document_id:\n",
    "        top_document.append(document_id)\n",
    "    return top_document\n",
    "\n",
    "def prepare_test_data(test_corpus):\n",
    "    final_data = []\n",
    "    for item in test_data:\n",
    "        question = item['question']\n",
    "        docid = item['docid']\n",
    "        processed_question = word_tokenize(question)\n",
    "        document = get_paragraph(docid,docs_corpus)\n",
    "        tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "        tfidf = get_okapibm25(tfs, total_docment,document)\n",
    "        top_1 = get_top_k_document(tfidf,question,1,document)\n",
    "        for item in top_1:\n",
    "            print(\"top_1\", top_1)\n",
    "            story = doc_to_story(top_1, docid)\n",
    "            print(\"story\",story)\n",
    "        final_data.append((story, processed_question, answer))\n",
    "        #print(\"final_data\",final_data)\n",
    "    return \n",
    "\n",
    "def prepare_test_del(train_corpus):\n",
    "    final_data = []\n",
    "    print(len(train_corpus))\n",
    "    for i in range(len(train_corpus)):\n",
    "        story = []\n",
    "        question = train_corpus[i]['question']\n",
    "        processed_question, answer, para_id, docid = qestion_and_answer(i)\n",
    "        document = get_paragraph(docid,docs_corpus)\n",
    "        tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "        tfidf = get_okapibm25(tfs, total_docment,document)\n",
    "        top_1 = get_top_k_document(tfidf,question,1,document)\n",
    "        for item in top_1:\n",
    "            #print(\"item\", item)\n",
    "            story = doc_to_story(item, docid)\n",
    "            #print(\"story\",story)\n",
    "        final_data.append((story, processed_question, answer))\n",
    "        #print(\"final_data\",final_data)\n",
    "    return final_data\n",
    "\n",
    "del_data = prepare_data(test_for_del)\n",
    "print(\"len(del_data)\",len(del_data))\n",
    "\n",
    "def get_vocab(data):\n",
    "    vocab = set()\n",
    "    for story, q, answer in data:\n",
    "        list_words = []\n",
    "        for sent in story:            \n",
    "            list_words = list_words + sent \n",
    "        vocab_list = list_words + q + word_tokenize(answer)\n",
    "        #print(\"vocab_list\",vocab_list)\n",
    "        vocab |= set(vocab_list)\n",
    "    vocab = sorted(vocab)\n",
    "    return vocab\n",
    "vocab = get_vocab(data + del_data)\n",
    "#print(\"vocab\",vocab)\n",
    "print(\"len(vocab)\",len(vocab))\n",
    "\n",
    "\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in data + del_data)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in data + del_data)))\n",
    "\n",
    "x, xq, y = vectorize_stories(data, word_idx, story_maxlen, query_maxlen)\n",
    "tx, txq, ty = vectorize_stories(del_data, word_idx, story_maxlen, query_maxlen)\n",
    "\n",
    "#print('vocab = {}'.format(vocab))\n",
    "print('x.shape = {}'.format(x.shape))\n",
    "print('xq.shape = {}'.format(xq.shape))\n",
    "print('y.shape = {}'.format(y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "\n",
    "sentence = layers.Input(shape=(story_maxlen,), dtype='int32')\n",
    "encoded_sentence = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence)\n",
    "encoded_sentence = layers.Dropout(0.3)(encoded_sentence)\n",
    "\n",
    "question = layers.Input(shape=(query_maxlen,), dtype='int32')\n",
    "encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\n",
    "encoded_question = layers.Dropout(0.3)(encoded_question)\n",
    "encoded_question = RNN(EMBED_HIDDEN_SIZE)(encoded_question)\n",
    "encoded_question = layers.RepeatVector(story_maxlen)(encoded_question)\n",
    "\n",
    "merged = layers.add([encoded_sentence, encoded_question])\n",
    "merged = RNN(EMBED_HIDDEN_SIZE)(merged)\n",
    "merged = layers.Dropout(0.3)(merged)\n",
    "preds = layers.Dense(vocab_size, activation='softmax')(merged)\n",
    "\n",
    "model = Model([sentence, question], preds)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training')\n",
    "model.fit([x, xq], y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.05)\n",
    "loss, acc = model.evaluate([tx, txq], ty,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
