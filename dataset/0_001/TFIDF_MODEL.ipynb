{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\nltk\\tag\\stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n",
      "G:\\Anaconda\\lib\\site-packages\\nltk\\tag\\stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n",
      "G:\\Anaconda\\lib\\site-packages\\nltk\\tag\\stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#firsr of all import all package and load the file\n",
    "import string\n",
    "from math import log\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords    \n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "jar = 'stanford-ner.jar'\n",
    "model = 'english.conll.4class.distsim.crf.ser.gz'\n",
    "model1 = 'english.all.3class.distsim.crf.ser.gz'\n",
    "model2 = 'english.muc.7class.distsim.crf.ser.gz'\n",
    "punc = string.punctuation\n",
    "stopwordsPart = set(stopwords.words('english'))\n",
    "stopwordsPart.remove('the')  \n",
    "stopwordsPart.remove('of') \n",
    "stopwordsAll = set(stopwords.words('english'))\n",
    "def opne_json(text):\n",
    "    with open(text,'r') as input_file:\n",
    "        document = json.load(input_file)\n",
    "    return document\n",
    "\n",
    "\n",
    "\n",
    "def get_tag_model(model,jar):\n",
    "    return StanfordNERTagger(model,jar)\n",
    "\n",
    "person_model = get_tag_model(model,jar)\n",
    "person_model2 = get_tag_model(model1,jar)\n",
    "number_model = get_tag_model(model2,jar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "documents_dict = opne_json(\"documents.json\")\n",
    "test_dict = opne_json(\"testing.json\")\n",
    "dev_dict = opne_json(\"devel.json\")\n",
    "train_dict = opne_json(\"training.json\")\n",
    "query_lables = opne_json(\"QuestionLabel.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named spacy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-338396fa7807>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspatial\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named spacy"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import spacy\n",
    "from scipy import spatial\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim import models\n",
    "import string\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    " \n",
    "import hashlib\n",
    "import re\n",
    "import itertools\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    words = [lemmatize(removePunc(x.lower())) for x in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def named_entity_recognize(sentence,valid_ner):\n",
    "    #do not preprocess sentence when getting named entity using spacy\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    ner_tag = []\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(sentence)\n",
    "    if valid_ner == 'disable':\n",
    "        ner_tag = ''\n",
    "        for ent in doc.ents:\n",
    "            ner_tag=ent.label_\n",
    "        if ner_tag == '':\n",
    "            ner_tag = 'O'\n",
    "        return ner_tag\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in valid_ner:\n",
    "            entities.append(ent)\n",
    "            ner_tag.append(ent.label_)\n",
    "    # case 'O'\n",
    "    if valid_ner == 'O':\n",
    "        sentence = preprocess_sentence(sentence)\n",
    "        ents = sentence.split(' ')\n",
    "        \n",
    "        \"\"\"\n",
    "        add more selection method\n",
    "        \"\"\"\n",
    "        entities += ents\n",
    "        ner_tag.append('O')\n",
    "        if entities == []:\n",
    "            return 'unknown'\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get paragraph from the training data\n",
    "def get_paragraph(docid,documents_dict):\n",
    "    #get the paragraph that contains the answer\n",
    "    for i in documents_dict:\n",
    "        if i['docid'] == docid:\n",
    "            document = i['text']\n",
    "            break\n",
    "    return document\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get TF \n",
    "def term_freqs(document):\n",
    "    tfs = defaultdict(dict)\n",
    "    tfs_forward = defaultdict(dict)\n",
    "    doc_id = 0\n",
    "    for sentence in document:\n",
    "        for token in word_tokenize(sentence):\n",
    "            if token not in stopwordsAll and token not in punc:  \n",
    "                term = lemmatizer.lemmatize(token.lower())\n",
    "                tfs[term][doc_id] = tfs[term].get(doc_id, 0) + 1 \n",
    "                tfs_forward[doc_id][term] = tfs[doc_id].get(term, 0) + 1 \n",
    "        doc_id += 1\n",
    "    return tfs,doc_id+1,tfs_forward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build TF_IDF model\n",
    "def get_tfidf(tfs, total_docment,tfs_forward):\n",
    "    document_length = {}\n",
    "    for doc_id,doc_list in tfs_forward.items():\n",
    "        length = 0\n",
    "        for term, freq in doc_list.items():\n",
    "            length += freq ** 2\n",
    "        length = length **0.5\n",
    "        document_length[doc_id] =  length\n",
    "    tfidf = defaultdict(dict)\n",
    "    for term, doc_list in tfs.items():\n",
    "        df = len(doc_list)\n",
    "        for doc_id, freq in doc_list.items(): \n",
    "            tfidf[term][doc_id] = (float(tfs[term][doc_id]) * log(total_docment / df))# / document_length[doc_id]\n",
    "    return tfidf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find top_k paragraph that may contain the answer\n",
    "def get_top_k_document(tfidf,query,k,document):\n",
    "    top_document_id = Counter()\n",
    "    for token in word_tokenize(query):\n",
    "        if token not in stopwordsAll:  \n",
    "             term = lemmatizer.lemmatize(token.lower())\n",
    "             term_tfidf = tfidf[term]\n",
    "             for docid, weight in term_tfidf.items():\n",
    "                 top_document_id[docid] += weight\n",
    "    top_document_id = top_document_id.most_common(k)\n",
    "    top_document = []\n",
    "    for document_id,weight in top_document_id:\n",
    "        top_document.append(document[document_id])\n",
    "    return top_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the key words in query\n",
    "def get_open_class_word(query):\n",
    "    tagged = nltk.pos_tag(word_tokenize(query), tagset=\"universal\")\n",
    "    return [p[0] for p in tagged if (p[1] in [\"NOUN\",\"VERB\"] or p[0].isdigit()) and p[0] not in [\"did\",\"do\",\"was\",\"were\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the NER with same tag\n",
    "def same_tag(ner_output):\n",
    "    word,tag = '',''\n",
    "    combo = []\n",
    "    for word1,tag1 in ner_output:\n",
    "        if tag1 == tag:\n",
    "            if word[-1] in ['(',')']:\n",
    "                word += word1\n",
    "            if word1 in [')']:\n",
    "                 word += word1\n",
    "            else:     \n",
    "                word += \" \" + word1\n",
    "        else:\n",
    "            combo.append((word,tag))\n",
    "            tag = tag1\n",
    "            word = word1\n",
    "            continue\n",
    "    if len(combo) != 0:\n",
    "        combo.pop(0)\n",
    "    return combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_in(key_words,sentence):\n",
    "    all_in = True\n",
    "    len1 = len(key_words)\n",
    "    word_in = 0\n",
    "    for i in key_words:\n",
    "        try:\n",
    "            index = sentence.index(i)\n",
    "            word_in += 1 \n",
    "        except ValueError:\n",
    "            continue\n",
    "    return len1 < 2*word_in\n",
    "\n",
    "def in_key_words(word,key_words):\n",
    "    in_key = False\n",
    "    for i in key_words:\n",
    "        if word.find(i) != -1:       \n",
    "            in_key = True\n",
    "            break\n",
    "    return in_key\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyoncé further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records. Her performance in the film received praise from critics, and she garnered several nominations for her portrayal of James, including a Satellite Award nomination for Best Supporting Actress, and a NAACP Image Award nomination for Outstanding Supporting Actress. Beyoncé donated her entire salary from the film to Phoenix House, an organization of rehabilitation centers for heroin addicts around the country. On January 20, 2009, Beyoncé performed James' \"At Last\" at the First Couple's first inaugural ball. Beyoncé starred opposite Ali Larter and Idris Elba in the thriller, Obsessed. She played Sharon Charles, a mother and wife who learns of a woman's obsessive behavior over her husband. Although the film received negative reviews from critics, the movie did well at the US box office, grossing $68 million—$60 million more than Cadillac Records—on a budget of $20 million. The fight scene finale between Sharon and the character played by Ali Larter also won the 2010 MTV Movie Award for Best Fight.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def get_sentence(train_dict,documents_dict):\n",
    "    #get the paragraph that contains the answer\n",
    "    document = []\n",
    "    docid = 379\n",
    "    answer_paragraph = 15\n",
    "    answer_sentence = documents_dict[docid]['text'][answer_paragraph]\n",
    "       \n",
    "    return answer_sentence\n",
    "answer_sentence = get_sentence(train_dict,documents_dict)\n",
    "print answer_sentence\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = get_paragraph(409,documents_dict)\n",
    "tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"At this time, Hal B. Wallis, who had latterly worked as a major producer at Paramount, moved over to Universal, where he produced several films, among them a lavish version of Maxwell Anderson's Anne of the Thousand Days (1969), and the equally lavish Mary, Queen of Scots (1971). Though neither could claim to be a big financial hit, both films received Academy Award nominations, and Anne was nominated for Best Picture, Best Actor (Richard Burton), Best Actress (Genevi\\xe8ve Bujold), and Best Supporting Actor (Anthony Quayle). Wallis retired from Universal after making the film Rooster Cogburn (1975), a sequel to True Grit (1969), which Wallis had produced at Paramount. Rooster Cogburn co-starred John Wayne, reprising his Oscar-winning role from the earlier film, and Katharine Hepburn, their only film together. The film was only a moderate success.\", u\"The production arm of the studio still struggled. While there were to be a few hits like The Killers (1946) and The Naked City (1948), Universal-International's new theatrical films often met with disappointing response at the box office. By the late 1940s, Goetz was out, and the studio returned to low-budget films. The inexpensive Francis (1950), the first film of a series about a talking mule and Ma and Pa Kettle (1949), part of a series, became mainstays of the company. Once again, the films of Abbott and Costello, including Abbott and Costello Meet Frankenstein (1948), were among the studio's top-grossing productions. But at this point Rank lost interest and sold his shares to the investor Milton Rackmil, whose Decca Records would take full control of Universal in 1952. Besides Abbott and Costello, the studio retained the Walter Lantz cartoon studio, whose product was released with Universal-International's films.\", u\"Though Decca would continue to keep picture budgets lean, it was favored by changing circumstances in the film business, as other studios let their contract actors go in the wake of the 1948 U.S. vs. Paramount Pictures, et al. decision. Leading actors were increasingly free to work where and when they chose, and in 1950 MCA agent Lew Wasserman made a deal with Universal for his client James Stewart that would change the rules of the business. Wasserman's deal gave Stewart a share in the profits of three pictures in lieu of a large salary. When one of those films, Winchester '73, proved to be a hit, the arrangement would become the rule for many future productions at Universal, and eventually at other studios as well.\", u'\"Junior\" Laemmle persuaded his father to bring Universal up to date. He bought and built theaters, converted the studio to sound production, and made several forays into high-quality production. His early efforts included the critically mauled part-talkie version of Edna Ferber\\'s novel Show Boat (1929), the lavish musical Broadway (1929) which included Technicolor sequences; and the first all-color musical feature (for Universal), King of Jazz (1930). The more serious All Quiet on the Western Front (1930), won its year\\'s Best Picture Oscar.', u\"In the early 1970s, Universal teamed up with Paramount Pictures to form Cinema International Corporation, which distributed films by Paramount and Universal worldwide. Though Universal did produce occasional hits, among them Airport (1970), The Sting (1973), American Graffiti (also 1973), Earthquake (1974), and a big box-office success which restored the company's fortunes: Jaws (1975), Universal during the decade was primarily a television studio. When Metro-Goldwyn-Mayer purchased United Artists in 1981, MGM could not drop out of the CIC venture to merge with United Artists overseas operations. However, with future film productions from both names being released through the MGM/UA Entertainment plate, CIC decided to merge UA's international units with MGM and reformed as United International Pictures. There would be other film hits like E.T. the Extra-Terrestrial (1982), Back to the Future (1985), Field of Dreams (1989), and Jurassic Park (1993), but the film business was financially unpredictable. UIP began distributing films by start-up studio DreamWorks in 1997, due to connections the founders have with Paramount, Universal, and Amblin Entertainment. In 2001, MGM dropped out of the UIP venture, and went with 20th Century Fox's international arm to handle distribution of their titles to this day.\"]\n"
     ]
    }
   ],
   "source": [
    "query =\"Whose performance in Anne of the Thousand Days garnered a Best Actor nod?\"\n",
    "top_k = get_top_k_document(tfidf,query,5,document)\n",
    "print top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_answer_list(query,answer_type):\n",
    "    key_words = get_open_class_word(query)\n",
    "    print key_words\n",
    "    answer_list = {}\n",
    "    answer_type = answer_type\n",
    "    for ans_sentence in top_k:\n",
    "        if most_in(key_words,ans_sentence) == False:\n",
    "            continue\n",
    "        word_list =  []\n",
    "        for word in word_tokenize(ans_sentence):\n",
    "            word_list.append(word)    \n",
    "        word_list_tag = number_model.tag(word_list)\n",
    "        word_list_tag = same_tag(word_list_tag)\n",
    "        for word,tags in word_list_tag:\n",
    "            if word not in answer_list.keys():\n",
    "                if word not in stopwordsAll and word not in punc and tags == answer_type and word not in key_words and in_key_words(word,key_words) == False:\n",
    "                    distance_list = []\n",
    "                    distance = 0\n",
    "                    for key_word in key_words:\n",
    "                        try:\n",
    "                            index = ans_sentence.index(key_word)\n",
    "                            distance_list.append(index)\n",
    "                        except ValueError:\n",
    "                            distance_list.append(5000)\n",
    "                    for index in distance_list:\n",
    "                        try:\n",
    "                            distance += abs(index - ans_sentence.index(word))\n",
    "                        except ValueError:\n",
    "                            print '2'\n",
    "                    answer_list[word] = distance\n",
    "    if  len(answer_list.items()) != 0:\n",
    "        return sorted(answer_list.items(), lambda x, y: cmp(x[1], y[1]))#[0][0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['performance', 'Anne', 'Thousand', 'Days', 'garnered', 'Best', 'Actor', 'nod']\n",
      "[(u'Richard Burton', 14415), (u'Genevi\\xe8ve Bujold)', 14477), (u'Anthony Quayle', 14569), (u'Mary', 14715), (u'John Wayne', 14949), (u'Maxwell Anderson', 15047), (u'Katharine Hepburn', 15093), (u'Wallis', 16287), (u'Hal B. Wallis', 16343)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print get_answer_list(query,\"PERSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_answer_list() takes exactly 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-370f18fdca51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_lables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-370f18fdca51>\u001b[0m in \u001b[0;36moutput\u001b[1;34m(labels)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question_answer_type_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0manswer_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question_answer_type_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_answer_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mget_top_k_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manswer_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mqa_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_answer_list() takes exactly 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def output(labels):\n",
    "    qa_id = 0\n",
    "    csv_file = open('output.csv', 'w')\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['id', 'answer'])\n",
    "    for i in labels:\n",
    "        query =  i['question_answer_type_dict'].keys()[0]\n",
    "        answer_type = i['question_answer_type_dict'].values()[0]\n",
    "        answer = get_answer_list(query,get_top_k_document(tfidf,query,10,document),answer_type)\n",
    "        print answer\n",
    "        writer.writerow([qa_id, answer])\n",
    "        qa_id += 1\n",
    "    csv_file.close()\n",
    "'''\n",
    "    for article in data:\n",
    "        collection = article['sentences']\n",
    "        tf, total_docs = get_term_frequencies(collection)\n",
    "        #tfidf = get_tfidf(tf, total_docs)\n",
    "        tagged_sentences = rechunk(entity_recognize(collection))       \n",
    "        for qa in article['qa']:\n",
    "            question = qa['question']\n",
    "            qa_id = qa['id']\n",
    "            okapibm25 = get_okapibm25(tf, total_docs, collection)\n",
    "            sentence_id = retrieve_sentence(okapibm25, question)\n",
    "            if len(sentence_id) == 0:\n",
    "                writer.writerow([qa_id, 'Not sure'])\n",
    "            else:\n",
    "                doc_id = sentence_id[0][0]\n",
    "                first_pass_result = first_pass(tagged_sentences[doc_id], question, collection[doc_id])\n",
    "                if first_pass_result == []:\n",
    "                    writer.writerow([qa_id, 'Not sure'])\n",
    "                else:\n",
    "                    second_pass_result = second_pass(first_pass_result, question)\n",
    "                    third_pass_result = third_pass(question, second_pass_result)\n",
    "                    writer.writerow([qa_id, third_pass_result])\n",
    "'''\n",
    "    \n",
    "\n",
    "output(query_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "2856\n",
      "3097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9221827575072651"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_accuracy(train_dict,documents_dict):\n",
    "    total = 0\n",
    "    right = 0\n",
    "    docidstart = train_dict[0]['docid']\n",
    "    document = get_paragraph(docidstart,documents_dict)\n",
    "    tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "    tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n",
    "    for i in train_dict:\n",
    "        docid = i['docid']\n",
    "        if docid != docidstart:\n",
    "            print docid\n",
    "            document = get_paragraph(docid,documents_dict)\n",
    "            tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "            tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n",
    "            docidstart = docid\n",
    "        query = i['question']\n",
    "        answer_paragraph = i['answer_paragraph']\n",
    "        answer_sentence = documents_dict[docid]['text'][answer_paragraph]\n",
    "        top_k = get_top_k_document(tfidf,query,5,document)\n",
    "        if answer_sentence in top_k:\n",
    "            right += 1\n",
    "        total += 1\n",
    "    print right\n",
    "    print total\n",
    "    return float(right)/total\n",
    "\n",
    "get_accuracy(dev_dict,documents_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_list(query):\n",
    "    key_words = get_open_class_word(query)\n",
    "    print key_words\n",
    "    answer_list = {}\n",
    "    answer_type = answer_type\n",
    "    for ans_sentence in top_k:\n",
    "        if most_in(key_words,ans_sentence) == False:\n",
    "            continue\n",
    "        word_list_tag = named_entity_recognize(ans_sentence,answer_type)\n",
    "        for word,tags in word_list_tag:\n",
    "            if word not in answer_list.keys():\n",
    "                if word not in stopwordsAll and word not in punc and tags == answer_type and word not in key_words and in_key_words(word,key_words) == False:\n",
    "                    distance_list = []\n",
    "                    distance = 0\n",
    "                    for key_word in key_words:\n",
    "                        try:\n",
    "                            index = ans_sentence.index(key_word)\n",
    "                            distance_list.append(index)\n",
    "                        except ValueError:\n",
    "                            distance_list.append(5000)\n",
    "                    for index in distance_list:\n",
    "                        try:\n",
    "                            distance += abs(index - ans_sentence.index(word))\n",
    "                        except ValueError:\n",
    "                            print '2'\n",
    "                    answer_list[word] = distance\n",
    "    if  len(answer_list.items()) != 0:\n",
    "        return sorted(answer_list.items(), lambda x, y: cmp(x[1], y[1]))#[0][0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print get_answer_list(query,\"PERSON\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
