{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from math import log\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords    \n",
    "from nltk.tag import StanfordNERTagger\n",
    "import json\n",
    "import nltk\n",
    "import csv\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "jar = 'stanford-ner.jar'\n",
    "model = 'english.conll.4class.distsim.crf.ser.gz'\n",
    "model1 = 'english.all.3class.distsim.crf.ser.gz'\n",
    "model2 = 'english.muc.7class.distsim.crf.ser.gz'\n",
    "punc = string.punctuation\n",
    "stopwordsPart = set(stopwords.words('english'))\n",
    "stopwordsPart.remove('the')  \n",
    "stopwordsPart.remove('of') \n",
    "stopwordsAll = set(stopwords.words('english'))\n",
    "\n",
    "def opne_json(text):\n",
    "    with open(text,'r') as input_file:\n",
    "        document = json.load(input_file)\n",
    "    return document\n",
    "\n",
    "\n",
    "\n",
    "def get_tag_model(model,jar):\n",
    "    return StanfordNERTagger(model,jar)\n",
    "\n",
    "person_model = get_tag_model(model,jar)\n",
    "person_model2 = get_tag_model(model1,jar)\n",
    "number_model = get_tag_model(model2,jar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "documents_dict = opne_json(\"documents.json\")\n",
    "test_dict = opne_json(\"testing.json\")\n",
    "dev_dict = opne_json(\"devel.json\")\n",
    "train_dict = opne_json(\"training.json\")\n",
    "query_lables = opne_json(\"QuestionLabel.json\")\n",
    "\n",
    "\n",
    "#get paragraph from the training data\n",
    "def get_paragraph(docid,documents_dict):\n",
    "    #get the paragraph that contains the answer\n",
    "    for i in documents_dict:\n",
    "        if i['docid'] == docid:\n",
    "            document = i['text']\n",
    "            break\n",
    "    return document\n",
    "\n",
    "\n",
    "#get TF \n",
    "def term_freqs(document):\n",
    "    tfs = defaultdict(dict)\n",
    "    tfs_forward = defaultdict(dict)\n",
    "    doc_id = 0\n",
    "    for sentence in document:\n",
    "        for token in word_tokenize(sentence):\n",
    "            if token not in stopwordsAll and token not in punc:  \n",
    "                term = lemmatizer.lemmatize(token.lower())\n",
    "                tfs[term][doc_id] = tfs[term].get(doc_id, 0) + 1 \n",
    "                tfs_forward[doc_id][term] = tfs[doc_id].get(term, 0) + 1 \n",
    "        doc_id += 1\n",
    "    return tfs,doc_id+1,tfs_forward\n",
    "\n",
    "#build TF_IDF model\n",
    "def get_tfidf(tfs, total_docment,tfs_forward):\n",
    "    document_length = {}\n",
    "    for doc_id,doc_list in tfs_forward.items():\n",
    "        length = 0\n",
    "        for term, freq in doc_list.items():\n",
    "            length += freq ** 2\n",
    "        length = length **0.5\n",
    "        document_length[doc_id] =  length\n",
    "    tfidf = defaultdict(dict)\n",
    "    for term, doc_list in tfs.items():\n",
    "        df = len(doc_list)\n",
    "        for doc_id, freq in doc_list.items(): \n",
    "            tfidf[term][doc_id] = (float(tfs[term][doc_id]) * log(total_docment / df))# / document_length[doc_id]\n",
    "    return tfidf\n",
    "\n",
    "def get_okapibm25(tf, total_docment, documents):\n",
    "    '''Calculate and return term weights based on okapibm25'''\n",
    "    k1, b, k3 = 1.5, 0.5, 0\n",
    "    okapibm25 = defaultdict(dict)\n",
    "\n",
    "    # calculate average doc length \n",
    "    total = 0\n",
    "    for d in documents:\n",
    "        total += len(d)\n",
    "    avg_doc_length = total/len(documents)*1.0\n",
    "\n",
    "    for term, doc_list in tf.items():\n",
    "        df = len(doc_list)\n",
    "        for doc_id, freq in doc_list.items():\n",
    "            # term occurences in query\n",
    "            # qtf = question.count(term) # SEPCIAL \n",
    "            qtf = 1.2\n",
    "            idf = log((total_docment-df+0.5) / df+0.5)\n",
    "            tf_Dt = ((k1+1)*tf[term][doc_id]) / (k1*((1-b)+b*(len(documents[doc_id])/avg_doc_length) + tf[term][doc_id]))\n",
    "            if qtf == 0:\n",
    "                third = 0\n",
    "            else:\n",
    "                third = ((k3+1)*qtf) / (k3+qtf)\n",
    "                okapibm25[term][doc_id] = idf*tf_Dt*third\n",
    "\n",
    "    return okapibm25\n",
    "\n",
    "#find top_k paragraph that may contain the answer\n",
    "def get_top_k_document(tfidf,query,k,document):\n",
    "    top_document_id = Counter()\n",
    "    for token in word_tokenize(query):\n",
    "        if token not in stopwordsAll:  \n",
    "             term = lemmatizer.lemmatize(token.lower())\n",
    "             term_tfidf = tfidf[term]\n",
    "             for docid, weight in term_tfidf.items():\n",
    "                 top_document_id[docid] += weight\n",
    "    top_document_id = top_document_id.most_common(k)\n",
    "    top_document = []\n",
    "    for document_id,weight in top_document_id:\n",
    "        top_document.append(document[document_id])\n",
    "    return top_document\n",
    "#filter the key words in query\n",
    "#filter the key words in query\n",
    "def get_open_class_word(query):\n",
    "    #query = nltk.word_tokenize(query)\n",
    "    #tagged = nltk.pos_tag(query)#nltk.word_tokenize(query))#, tagset=\"universal\")\n",
    "    tagged = nltk.pos_tag(word_tokenize(query), tagset=\"universal\")\n",
    "    return [p[0] for p in tagged if p[1] in [\"NOUN\",\"VERB\",\"NUM\"] and p[0] not in stopwordsAll]\n",
    "    #return [p[0] for p in tagged if p[1] in [\"NN\",\"NNP\",\"NNS\",\"NP\",\"VB\",\"VBD\",\"CD\",\"JJ\",] and p[0] not in stopwordsAll]\n",
    "query = \"of\"\n",
    "query = nltk.word_tokenize(query)\n",
    "tagged = nltk.pos_tag(query)\n",
    "print tagged\n",
    "#combine the NER with same tag\n",
    "def same_tag(ner_output):\n",
    "    word,tag = 'the','O'\n",
    "    combo = []\n",
    "    for word1,tag1 in ner_output:\n",
    "        '''\n",
    "        if tag1 == \"O\" and word1 not in stopwordsAll and word1 not in punc:\n",
    "            combo.append((word,tag))\n",
    "            tag = tag1\n",
    "            word = word1\n",
    "            continue\n",
    "            '''\n",
    "        if tag1 == tag:\n",
    "            if word[-1] in ['(',')']:\n",
    "                word += word1\n",
    "            if word1 in [')']:\n",
    "                 word += word1\n",
    "            else:     \n",
    "                word += \" \" + word1\n",
    "        else:\n",
    "            combo.append((word,tag))\n",
    "            tag = tag1\n",
    "            word = word1\n",
    "            continue\n",
    "    if len(combo) != 0:\n",
    "        combo.pop(0)\n",
    "    return combo\n",
    "\n",
    "\n",
    "def same_tag_other(ner_output):\n",
    "    word,tag = 'the','O'\n",
    "    combo = []\n",
    "    for word1,tag1 in ner_output:\n",
    "        '''\n",
    "        if tag1 == \"O\" and word1 not in stopwordsAll and word1 not in punc:\n",
    "            combo.append((word,tag))\n",
    "            tag = tag1\n",
    "            word = word1\n",
    "            continue\n",
    "            '''\n",
    "        if tag1 in [\"NOUN\",\"ADJ\",\"NUM\"] and tag in [\"NOUN\"]:\n",
    "        #if tag in [\"NN\",\"NNP\",\"JJ\",\"CD\",\"CC\",\"NNS\",\"NP\",\"IN\"] and tag1 in [\"NN\",\"NNP\",\"JJ\",\"CD\",\"CC\",\"NNS\",\"NP\",\"IN\"]:\n",
    "        #if tag in [\"NN\",\"NNP\",\"JJ\",\"CD\",\"NNS\",\"NNPS\"] and tag1 in [\"NN\",\"NNP\",\"NNS\",\"NNPS\"]:\n",
    "            if word[-1] in ['(',')']:\n",
    "                word += word1\n",
    "            if word1 in [')']:\n",
    "                 word += word1\n",
    "            else:     \n",
    "                word += \" \" + word1\n",
    "            tag = tag1\n",
    "        else:\n",
    "            combo.append((word,tag))\n",
    "            tag = tag1\n",
    "            word = word1\n",
    "            continue\n",
    "    if len(combo) != 0:\n",
    "        combo.pop(0)\n",
    "    return combo\n",
    "\n",
    "def most_in(key_words,sentence):\n",
    "    all_in = True\n",
    "    len1 = len(key_words)\n",
    "    word_in = 0\n",
    "    for i in key_words:\n",
    "        try:\n",
    "            index = sentence.index(i)\n",
    "            word_in += 1 \n",
    "        except ValueError:\n",
    "            continue\n",
    "    return len1 < 2*word_in\n",
    "\n",
    "def in_key_words(word,key_words):\n",
    "    in_key = False\n",
    "    for i in key_words:\n",
    "        if word.find(i) != -1:       \n",
    "            in_key = True\n",
    "            break\n",
    "    return in_key\n",
    "            \n",
    "rules = {\n",
    "            'name':'PERSON',\n",
    "            'country': 'LOCATION',\n",
    "            'capital': 'LOCATION',\n",
    "            'newspaper':'ORGANIZATION',\n",
    "            'company':'ORGANIZATION',\n",
    "            'city': 'LOCATION',\n",
    "            'person':'PERSON',\n",
    "            'location': 'LOCATION',\n",
    "            'mountain':'LOCATION',\n",
    "            'website':'ORGANIZATION',\n",
    "            'airline':'ORGANIZATION',\n",
    "            'which organization': 'ORGANIZATION',\n",
    "            'where': 'LOCATION',\n",
    "            'when': 'DATE',           \n",
    "            'who': 'PERSON',     \n",
    "            'what scientist':'PERSON',\n",
    "            'what time':'TIME',\n",
    "            'what athlete':'PERSON',\n",
    "            'which athlete':'PERSON',\n",
    "            'what people': 'PERSON',\n",
    "            'what date':'DATE',\n",
    "            'what day':'DATE',\n",
    "            'what year': 'DATE',\n",
    "            'what city' : 'LOCATION',\n",
    "            'which company': 'ORGANIZATION' ,\n",
    "            'which publication':'ORGANIZATION',\n",
    "            'what government':'ORGANIZATION',\n",
    "            'which supporters' : 'PERSON',\n",
    "            'which footballer': 'PERSON',\n",
    "            'which actor':'PERSON',\n",
    "            'Which actress':'PERSON',\n",
    "            'which American actress':'PERSON',\n",
    "            'what activists':'PERSON',\n",
    "            'which team member' : 'PERSON',\n",
    "            'what football star': 'PERSON',\n",
    "            'which blogger': 'PERSON',\n",
    "            'which torchbearer':'PERSON',\n",
    "            'which wheelchair-bound torchbearer' : 'PERSON',\n",
    "            'how much of': 'PERCENT',\n",
    "            'by how much': 'PERCENT',\n",
    "            'how much': 'O'\n",
    "            \n",
    "            \n",
    "}\n",
    "\n",
    "money_list = ['cost', 'worth', 'spend', 'money', 'worth', 'invest']\n",
    "\n",
    "def tag_answer_type(question):\n",
    "    answer_type = 'O'\n",
    "    processed_question = []\n",
    "    processed_question_str = None\n",
    "    for token in [question]:\n",
    "        processed_question.append(token.lower())\n",
    "    processed_question_str = \" \".join(x for x in processed_question)\n",
    "    for k,v in rules.items():\n",
    "        if k in processed_question_str:\n",
    "            #print(k)\n",
    "            if k == 'how much':\n",
    "                for item in money_list:\n",
    "                    #print(\"item\", item)\n",
    "                    if item in processed_question_str:\n",
    "                        answer_type = 'MONEY'\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "                answer_type = rules.get(k, \"O\")    \n",
    "    return answer_type\n",
    "\n",
    "def get_answer_list(query,top_k):\n",
    "    key_words = get_open_class_word(query)\n",
    "    answer_list = {}\n",
    "    answer_type = tag_answer_type(query)\n",
    "    for ans_sentence in top_k:\n",
    "        #if most_in(key_words,ans_sentence) == False:\n",
    "          #  continue\n",
    "        if answer_type == \"O\":\n",
    "            word_list = nltk.word_tokenize(ans_sentence)\n",
    "            word_list_tag = nltk.pos_tag(word_tokenize(ans_sentence), tagset=\"universal\")\n",
    "            word_list_tag = same_tag_other(word_list_tag)\n",
    "            answer_type = [\"NOUN\",\"NUM\"] \n",
    "            #word_list_tag = nltk.pos_tag(word_list)\n",
    "            #word_list_tag = same_tag_other(word_list_tag)\n",
    "            #answer_type = [\"NN\",\"NNP\",\"NNS\",\"NNPS\",\"CD\"]\n",
    "            '''\n",
    "            if \"how\" in query:\n",
    "                answer_type = [\"CD\"]\n",
    "            else:\n",
    "                answer_type = [\"NN\",\"NNP\",\"NNS\",\"NP\"]\n",
    "            '''    \n",
    "        else:\n",
    "            word_list =  []\n",
    "            for word in word_tokenize(ans_sentence):\n",
    "                word_list.append(word)    \n",
    "            word_list_tag = number_model.tag(word_list)\n",
    "            word_list_tag = same_tag(word_list_tag)\n",
    "            answer_type = [answer_type]\n",
    "        for word,tags in word_list_tag:\n",
    "            if word not in answer_list.keys():\n",
    "                if word not in stopwordsAll and word not in punc and tags in answer_type and word not in key_words and in_key_words(word,key_words) == False:\n",
    "                    distance_list = []\n",
    "                    distance = 0\n",
    "                    for key_word in key_words:\n",
    "                        try:\n",
    "                            index = ans_sentence.index(key_word)\n",
    "                            distance_list.append(index)\n",
    "                        except ValueError:\n",
    "                            distance_list.append(5000)\n",
    "                    for index in distance_list:\n",
    "                        try:\n",
    "                            distance += abs(index - ans_sentence.index(word))\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                    answer_list[word] = distance\n",
    "    if  len(answer_list.items()) != 0:\n",
    "        return sorted(answer_list.items(), lambda x, y: cmp(x[1], y[1]))[0][0].lower()\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_csv(test_dict,documents_dict):\n",
    "    '''\n",
    "    csv_file = open('outpur.csv', 'w')\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['id', 'answer'])\n",
    "    '''\n",
    "    output = open(\"out_new.txt\", \"w\")\n",
    "    docidstart = train_dict[0]['docid']\n",
    "    document = get_paragraph(docidstart,documents_dict)\n",
    "    tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "    #tfidf = get_okapibm25(tfs, total_docment,tfs_forward)\n",
    "    tfidf = get_okapibm25(tfs, total_docment,document) # 25 model\n",
    "    for i in test_dict:\n",
    "        docid = i['docid']\n",
    "        if docid != docidstart:\n",
    "            document = get_paragraph(docid,documents_dict)\n",
    "            tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "            #tfidf = get_okapibm25(tfs, total_docment,tfs_forward)\n",
    "            tfidf = get_okapibm25(tfs, total_docment,document)\n",
    "            docidstart = docid\n",
    "        query = i['question']\n",
    "        qaid = i['id']\n",
    "        top_k = get_top_k_document(tfidf,query,1,document)\n",
    "        potiential_answer = get_answer_list(query,top_k)\n",
    "        #potiential_answer = potiential_answer.encode('ascii', 'ignore').decode('ascii')\n",
    "        try:#.encode('utf-8')\n",
    "            output.write(str(potiential_answer)+\"\\n\")\n",
    "        except UnicodeEncodeError:\n",
    "            output.write(str(qaid) + '\\n')\n",
    "            print potiential_answer\n",
    "            print str(qaid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
