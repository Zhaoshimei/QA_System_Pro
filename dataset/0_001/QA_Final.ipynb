{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_corpus) 43379\n",
      "5000\n",
      "Finishing writing into train_file.json!\n",
      "len(data) 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef lemmatize(word):\\n    lemma = lemmatizer.lemmatize(word, \\'v\\')\\n    if lemma == word:\\n        lemma = lemmatizer.lemmatize(word, \\'n\\')\\n    return lemma\\n\\n\\ndef vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\\n    xs = []\\n    xqs = []\\n    ys = []\\n    for story, query, answer in data:\\n        story_list = []\\n        for sent in story:\\n            for w in sent:\\n                story_list.append(w)\\n        x = [word_idx[w] for w in story_list]\\n        xq = [word_idx[w] for w in query]\\n        # let\\'s not forget that index 0 is reserved\\n        y = np.zeros(len(word_idx) + 1)\\n        for w in answer:\\n            y[word_idx[w]] = 1\\n        xs.append(x)\\n        xqs.append(xq)\\n        ys.append(y)\\n    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)\\n\\n\\ndef get_paragraph(docid, document_data):\\n    # get the paragraph that contains the answer\\n    for item in document_data:\\n        if item[\\'docid\\'] == docid:\\n            document = item[\\'text\\']\\n            break\\n    return document\\n\\n\\ndef term_freqs(document):\\n    tfs = defaultdict(dict)\\n    tfs_forward = defaultdict(dict)\\n    doc_id = 0\\n    for sentence in document:\\n        for token in word_tokenize(sentence):\\n            if token not in stopwordsAll and token not in punc:\\n                term = lemmatize(token.lower())\\n                tfs[term][doc_id] = tfs[term].get(doc_id, 0) + 1\\n                tfs_forward[doc_id][term] = tfs[doc_id].get(term, 0) + 1\\n        doc_id += 1\\n    return tfs, doc_id + 1, tfs_forward\\n\\n\\ndef get_okapibm25(tf, total_docment, documents):\\n    k1, b, k3 = 1.5, 0.5, 0\\n    okapibm25 = defaultdict(dict)\\n\\n    # calculate average doc length\\n    total = 0\\n    for d in documents:\\n        total += len(d)\\n    avg_doc_length = total / len(documents) * 1.0\\n\\n    for term, doc_list in tf.items():\\n        df = len(doc_list)\\n        for doc_id, freq in doc_list.items():\\n            # term occurences in query\\n            # qtf = question.count(term) # SEPCIAL\\n            qtf = 1.2\\n            idf = log((total_docment - df + 0.5) / df + 0.5)\\n            tf_Dt = ((k1 + 1) * tf[term][doc_id]) / (\\n            k1 * ((1 - b) + b * (len(documents[doc_id]) / avg_doc_length) + tf[term][doc_id]))\\n            if qtf == 0:\\n                third = 0\\n            else:\\n                third = ((k3 + 1) * qtf) / (k3 + qtf)\\n                okapibm25[term][doc_id] = idf * tf_Dt * third\\n\\n    return okapibm25\\n\\n\\n# find top_k paragraph that may contain the answer\\ndef get_top_k_document(tfidf, query, k, document):\\n    top_document_id = Counter()\\n    for token in word_tokenize(query):\\n        if token not in stopwordsAll:\\n            term = lemmatizer.lemmatize(token.lower())\\n            term_tfidf = tfidf[term]\\n            for docid, weight in term_tfidf.items():\\n                top_document_id[docid] += weight\\n    top_document_id = top_document_id.most_common(k)\\n    top_document = []\\n    for document_id, weight in top_document_id:\\n        top_document.append(document_id)\\n    return top_document\\n\\ndef prepare_test_del(train_corpus):\\n    final_data = []\\n    print(len(train_corpus))\\n    for i in range(len(train_corpus)):\\n        story = []\\n        question = train_corpus[i][\\'question\\']\\n        processed_question, answer, para_id, docid = qestion_and_answer(i)\\n        document = get_paragraph(docid, docs_corpus)\\n        tfs, total_docment, tfs_forward = term_freqs(document)\\n        tfidf = get_okapibm25(tfs, total_docment, document)\\n        top_1 = get_top_k_document(tfidf, question, 1, document)\\n        for item in top_1:\\n            # print(\"item\", item)\\n            story = doc_to_story(item, docid)\\n            # print(\"story\",story)\\n        final_data.append((story, processed_question, answer))\\n        # print(\"final_data\",final_data)\\n    return final_data\\n\\ndef prepare_test_data(test_corpus):\\n    final_data = []\\n    for item in test_corpus:\\n        question = item[\\'question\\']\\n        docid = item[\\'docid\\']\\n        processed_question = word_tokenize(question)\\n        document = get_paragraph(docid,docs_corpus)\\n        tfs,total_docment,tfs_forward = term_freqs(document)\\n        tfidf = get_okapibm25(tfs, total_docment,document)\\n        top_1 = get_top_k_document(tfidf,question,1,document)\\n        for item in top_1:\\n            #print(\"top_1\", top_1)\\n            story = doc_to_story(item, docid)\\n            #print(\"story\",story)\\n        final_data.append((story, processed_question))\\n        #print(\"final_data\",final_data)\\n    return final_data\\n\\ndel_data = prepare_test_del(test_for_del)\\nprint(\"len(del_data)\", len(del_data))\\n\\ndef vectorize_stories_test(data, word_idx, story_maxlen, query_maxlen):\\n    xs = []\\n    xqs = []\\n    for story, query in data:\\n        story_list = []\\n        for sent in story:\\n            for w in sent:\\n                story_list.append(w)\\n        x = [word_idx[w] for w in story_list]\\n        xq = [word_idx[w] for w in query]\\n        xs.append(x)\\n        xqs.append(xq)\\n    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen)\\n\\nprint(\"len(test_for_predict)\",len(test_for_predict))\\ntest_data = prepare_test_data(test_for_predict)\\nprint(\"len(test_data)\",len(test_data))\\n\\n\\n\\nvocab = get_vocab(data,del_data,test_data)\\n# print(\"vocab\",vocab)\\nprint(\"len(vocab)\", len(vocab))\\n\\n\\n\\nRNN = recurrent.LSTM\\nEMBED_HIDDEN_SIZE = 50\\nWORD2VEC_EMBED_SIZE = 100\\nSENT_HIDDEN_SIZE = 100\\nQUERY_HIDDEN_SIZE = 100\\nBATCH_SIZE = 32\\nEPOCHS = 40\\n\\nWORD2VEC_EMBED_SIZE = 100\\nprint(\\'RNN / Embed / Sent / Query = {}, {}, {}, {}\\'.format(RNN,\\n                                                           EMBED_HIDDEN_SIZE,\\n                                                           SENT_HIDDEN_SIZE,\\n                                                           QUERY_HIDDEN_SIZE))\\n\\n# Reserve 0 for masking via pad_sequences\\nvocab_size = len(vocab) + 1\\nword2idx = dict((c, i + 1) for i, c in enumerate(vocab))\\nidx2word = dict((i + 1, c) for i, c in enumerate(vocab))\\nprint(\"len(word2idx)\",len(word2idx))\\nstory_maxlen = max(map(len, (x for x, _, _ in data)))\\nquery_maxlen = max(map(len, (x for _, x, _ in data)))\\n\\nx, xq, y = vectorize_stories(data, word2idx, story_maxlen, query_maxlen)\\ntx, txq, ty = vectorize_stories(del_data, word2idx, story_maxlen, query_maxlen)\\n\\nprint(\"Prepare test data...\")\\n\\ntest_story, test_question = vectorize_stories_test(test_data, word2idx, story_maxlen, query_maxlen)\\n\\ntest_question_data = [test_story, test_question]\\n\\ndel data\\ndel del_data\\ngc.collect()\\n\\n#word2vec\\nmodelname = \"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/mywvmodel.model\"\\nmodel = Word2Vec.load(modelname)  # 3个文件放在一起：Word60.model   Word60.model.syn0.npy   Word60.model.syn1neg.npy\\nprint(\"read model successful\")\\nembedding_weights = np.zeros((vocab_size, WORD2VEC_EMBED_SIZE))\\nfor word, index in word2idx.items():\\n    try:\\n        embedding_weights[index, :] = model[word.lower()]\\n    except KeyError:\\n        pass  # keep as zero (not ideal, but what else can we do?)\\n\\ndel model\\ndel word2idx\\ngc.collect()\\n\\n#former\\n\\n# print(\\'vocab = {}\\'.format(vocab))\\nprint(\\'x.shape = {}\\'.format(x.shape))\\nprint(\\'xq.shape = {}\\'.format(xq.shape))\\nprint(\\'y.shape = {}\\'.format(y.shape))\\nprint(\\'story_maxlen, query_maxlen = {}, {}\\'.format(story_maxlen, query_maxlen))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import json\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent, LSTM, Dropout, Merge, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model \n",
    "\n",
    "# In[9]:\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from math import log\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, RepeatVector\n",
    "import gc\n",
    "\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "docs_corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/documents.json\",'r') as docs:\n",
    "    docs_corpus = json.load(docs)\n",
    "\n",
    "train_corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/training.json\",'r') as training:\n",
    "    train_corpus = json.load(training)\n",
    "print(\"len(train_corpus)\",len(train_corpus))\n",
    "\n",
    "test_for_train = train_corpus[30000:35000]\n",
    "'''\n",
    "dev_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/devel.json\",'r') as devel:\n",
    "    dev_corpus = json.load(devel)\n",
    "\n",
    "test_for_del = dev_corpus[:2000]\n",
    "\n",
    "test_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/testing.json\",'r') as test:\n",
    "    test_corpus = json.load(test)\n",
    "    \n",
    "test_for_predict = test_corpus\n",
    "'''\n",
    "punc = string.punctuation\n",
    "stopwordsAll = set(stopwords.words('english'))\n",
    "\n",
    "def qestion_and_answer(question_id):\n",
    "    question = train_corpus[question_id]['question']\n",
    "    answer = train_corpus[question_id]['text']\n",
    "    processed_question = word_tokenize(question)\n",
    "    processed_answer = word_tokenize(answer)\n",
    "    para_id = train_corpus[question_id]['answer_paragraph']\n",
    "    docid = train_corpus[question_id]['docid']\n",
    "    # print(\"processed_question\",processed_question)\n",
    "    # print(\"answer\",answer)\n",
    "    # print(\"docid\",docid)\n",
    "    return processed_question, processed_answer, para_id,docid\n",
    "\n",
    "\n",
    "# qestion_and_answer(0)\n",
    "\n",
    "def doc_to_story(para_id,docid):\n",
    "    story = []\n",
    "\n",
    "    doc = docs_corpus[docid]\n",
    "    para = \"\"\n",
    "    for index, para_data in enumerate(doc['text']):\n",
    "        if index == para_id:\n",
    "            para = para_data\n",
    "        sents = sent_tokenize(para)\n",
    "        # print(\"sents\",sents)\n",
    "        for sent in sents:\n",
    "            tokens = word_tokenize(sent)\n",
    "            # print(\"tokens\",tokens)\n",
    "            story.append(tokens)\n",
    "    # print(\"story\",story)\n",
    "    return story\n",
    "\n",
    "\n",
    "# doc_to_story(0)\n",
    "\n",
    "# save [(story, question, answer)]\n",
    "def prepare_data(train_corpus):\n",
    "    final_data = []\n",
    "    print(len(train_corpus))\n",
    "    for i in range(len(train_corpus)):\n",
    "        processed_question, answer, para_id,docid = qestion_and_answer(i)\n",
    "        story = doc_to_story(para_id,docid)\n",
    "        final_data.append((story, processed_question, answer))\n",
    "        # print(\"final_data\",final_data)\n",
    "        \n",
    "    return final_data\n",
    "\n",
    "'''\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        story_list = []\n",
    "        for sent in story:\n",
    "            for w in sent:\n",
    "                story_list.append(w)\n",
    "        x = [word_idx[w] for w in story_list]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        for token in answer:\n",
    "            y[word_idx[token]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)\n",
    "\n",
    "\n",
    "def get_vocab(data,del_data,test_data):\n",
    "    vocab = set()\n",
    "    for story, q, answer in data:\n",
    "        list_words = []\n",
    "        for sent in story:\n",
    "            list_words = list_words + sent\n",
    "        vocab_list = list_words + q + answer\n",
    "        # print(\"vocab_list\",vocab_list)\n",
    "        vocab |= set(vocab_list)\n",
    "    for story, q, answer in del_data:\n",
    "        list_words = []\n",
    "        for sent in story:\n",
    "            list_words = list_words + sent\n",
    "        vocab_list = list_words + q + answer\n",
    "        # print(\"vocab_list\",vocab_list)\n",
    "        vocab |= set(vocab_list)\n",
    "    for story, q in test_data:\n",
    "        list_words = []\n",
    "        for sent in story:\n",
    "            list_words = list_words + sent\n",
    "        vocab_list = list_words + q\n",
    "        # print(\"vocab_list\",vocab_list)\n",
    "        vocab |= set(vocab_list)\n",
    "    vocab = sorted(vocab)\n",
    "    return vocab\n",
    "'''\n",
    "data = prepare_data(test_for_train)\n",
    "with open(\"train_file.json\",'a') as train_file:\n",
    "    json.dump(data, train_file)\n",
    "print(\"Finishing writing into train_file.json!\")\n",
    "print(\"len(data)\", len(data))\n",
    "\n",
    "'''\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word, 'n')\n",
    "    return lemma\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        story_list = []\n",
    "        for sent in story:\n",
    "            for w in sent:\n",
    "                story_list.append(w)\n",
    "        x = [word_idx[w] for w in story_list]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        for w in answer:\n",
    "            y[word_idx[w]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)\n",
    "\n",
    "\n",
    "def get_paragraph(docid, document_data):\n",
    "    # get the paragraph that contains the answer\n",
    "    for item in document_data:\n",
    "        if item['docid'] == docid:\n",
    "            document = item['text']\n",
    "            break\n",
    "    return document\n",
    "\n",
    "\n",
    "def term_freqs(document):\n",
    "    tfs = defaultdict(dict)\n",
    "    tfs_forward = defaultdict(dict)\n",
    "    doc_id = 0\n",
    "    for sentence in document:\n",
    "        for token in word_tokenize(sentence):\n",
    "            if token not in stopwordsAll and token not in punc:\n",
    "                term = lemmatize(token.lower())\n",
    "                tfs[term][doc_id] = tfs[term].get(doc_id, 0) + 1\n",
    "                tfs_forward[doc_id][term] = tfs[doc_id].get(term, 0) + 1\n",
    "        doc_id += 1\n",
    "    return tfs, doc_id + 1, tfs_forward\n",
    "\n",
    "\n",
    "def get_okapibm25(tf, total_docment, documents):\n",
    "    k1, b, k3 = 1.5, 0.5, 0\n",
    "    okapibm25 = defaultdict(dict)\n",
    "\n",
    "    # calculate average doc length\n",
    "    total = 0\n",
    "    for d in documents:\n",
    "        total += len(d)\n",
    "    avg_doc_length = total / len(documents) * 1.0\n",
    "\n",
    "    for term, doc_list in tf.items():\n",
    "        df = len(doc_list)\n",
    "        for doc_id, freq in doc_list.items():\n",
    "            # term occurences in query\n",
    "            # qtf = question.count(term) # SEPCIAL\n",
    "            qtf = 1.2\n",
    "            idf = log((total_docment - df + 0.5) / df + 0.5)\n",
    "            tf_Dt = ((k1 + 1) * tf[term][doc_id]) / (\n",
    "            k1 * ((1 - b) + b * (len(documents[doc_id]) / avg_doc_length) + tf[term][doc_id]))\n",
    "            if qtf == 0:\n",
    "                third = 0\n",
    "            else:\n",
    "                third = ((k3 + 1) * qtf) / (k3 + qtf)\n",
    "                okapibm25[term][doc_id] = idf * tf_Dt * third\n",
    "\n",
    "    return okapibm25\n",
    "\n",
    "\n",
    "# find top_k paragraph that may contain the answer\n",
    "def get_top_k_document(tfidf, query, k, document):\n",
    "    top_document_id = Counter()\n",
    "    for token in word_tokenize(query):\n",
    "        if token not in stopwordsAll:\n",
    "            term = lemmatizer.lemmatize(token.lower())\n",
    "            term_tfidf = tfidf[term]\n",
    "            for docid, weight in term_tfidf.items():\n",
    "                top_document_id[docid] += weight\n",
    "    top_document_id = top_document_id.most_common(k)\n",
    "    top_document = []\n",
    "    for document_id, weight in top_document_id:\n",
    "        top_document.append(document_id)\n",
    "    return top_document\n",
    "\n",
    "def prepare_test_del(train_corpus):\n",
    "    final_data = []\n",
    "    print(len(train_corpus))\n",
    "    for i in range(len(train_corpus)):\n",
    "        story = []\n",
    "        question = train_corpus[i]['question']\n",
    "        processed_question, answer, para_id, docid = qestion_and_answer(i)\n",
    "        document = get_paragraph(docid, docs_corpus)\n",
    "        tfs, total_docment, tfs_forward = term_freqs(document)\n",
    "        tfidf = get_okapibm25(tfs, total_docment, document)\n",
    "        top_1 = get_top_k_document(tfidf, question, 1, document)\n",
    "        for item in top_1:\n",
    "            # print(\"item\", item)\n",
    "            story = doc_to_story(item, docid)\n",
    "            # print(\"story\",story)\n",
    "        final_data.append((story, processed_question, answer))\n",
    "        # print(\"final_data\",final_data)\n",
    "    return final_data\n",
    "\n",
    "def prepare_test_data(test_corpus):\n",
    "    final_data = []\n",
    "    for item in test_corpus:\n",
    "        question = item['question']\n",
    "        docid = item['docid']\n",
    "        processed_question = word_tokenize(question)\n",
    "        document = get_paragraph(docid,docs_corpus)\n",
    "        tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "        tfidf = get_okapibm25(tfs, total_docment,document)\n",
    "        top_1 = get_top_k_document(tfidf,question,1,document)\n",
    "        for item in top_1:\n",
    "            #print(\"top_1\", top_1)\n",
    "            story = doc_to_story(item, docid)\n",
    "            #print(\"story\",story)\n",
    "        final_data.append((story, processed_question))\n",
    "        #print(\"final_data\",final_data)\n",
    "    return final_data\n",
    "\n",
    "del_data = prepare_test_del(test_for_del)\n",
    "print(\"len(del_data)\", len(del_data))\n",
    "\n",
    "def vectorize_stories_test(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    for story, query in data:\n",
    "        story_list = []\n",
    "        for sent in story:\n",
    "            for w in sent:\n",
    "                story_list.append(w)\n",
    "        x = [word_idx[w] for w in story_list]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen)\n",
    "\n",
    "print(\"len(test_for_predict)\",len(test_for_predict))\n",
    "test_data = prepare_test_data(test_for_predict)\n",
    "print(\"len(test_data)\",len(test_data))\n",
    "\n",
    "\n",
    "\n",
    "vocab = get_vocab(data,del_data,test_data)\n",
    "# print(\"vocab\",vocab)\n",
    "print(\"len(vocab)\", len(vocab))\n",
    "\n",
    "\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "WORD2VEC_EMBED_SIZE = 100\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "\n",
    "WORD2VEC_EMBED_SIZE = 100\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word2idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "idx2word = dict((i + 1, c) for i, c in enumerate(vocab))\n",
    "print(\"len(word2idx)\",len(word2idx))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in data)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in data)))\n",
    "\n",
    "x, xq, y = vectorize_stories(data, word2idx, story_maxlen, query_maxlen)\n",
    "tx, txq, ty = vectorize_stories(del_data, word2idx, story_maxlen, query_maxlen)\n",
    "\n",
    "print(\"Prepare test data...\")\n",
    "\n",
    "test_story, test_question = vectorize_stories_test(test_data, word2idx, story_maxlen, query_maxlen)\n",
    "\n",
    "test_question_data = [test_story, test_question]\n",
    "\n",
    "del data\n",
    "del del_data\n",
    "gc.collect()\n",
    "\n",
    "#word2vec\n",
    "modelname = \"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/mywvmodel.model\"\n",
    "model = Word2Vec.load(modelname)  # 3个文件放在一起：Word60.model   Word60.model.syn0.npy   Word60.model.syn1neg.npy\n",
    "print(\"read model successful\")\n",
    "embedding_weights = np.zeros((vocab_size, WORD2VEC_EMBED_SIZE))\n",
    "for word, index in word2idx.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = model[word.lower()]\n",
    "    except KeyError:\n",
    "        pass  # keep as zero (not ideal, but what else can we do?)\n",
    "\n",
    "del model\n",
    "del word2idx\n",
    "gc.collect()\n",
    "\n",
    "#former\n",
    "\n",
    "# print('vocab = {}'.format(vocab))\n",
    "print('x.shape = {}'.format(x.shape))\n",
    "print('xq.shape = {}'.format(xq.shape))\n",
    "print('y.shape = {}'.format(y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print('Build model...')\n",
    "\n",
    "qenc = Sequential()\n",
    "qenc.add(Embedding(output_dim=WORD2VEC_EMBED_SIZE, input_dim=vocab_size,\n",
    "                   weights=[embedding_weights], mask_zero=True))\n",
    "qenc.add(LSTM(EMBED_HIDDEN_SIZE, input_shape=(None,story_maxlen), return_sequences=False))\n",
    "qenc.add(Dropout(0.3))\n",
    "\n",
    "aenc = Sequential()\n",
    "aenc.add(Embedding(output_dim=WORD2VEC_EMBED_SIZE, input_dim=vocab_size,\n",
    "                   weights=[embedding_weights], mask_zero=True))\n",
    "aenc.add(LSTM(EMBED_HIDDEN_SIZE, input_shape=(None,query_maxlen), return_sequences=False))\n",
    "#aenc.add(RepeatVector(story_maxlen))\n",
    "aenc.add(Dropout(0.3))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([qenc, aenc], mode=\"sum\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "print('Training')\n",
    "\n",
    "model.fit([x, xq], y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.05)\n",
    "loss, acc = model.evaluate([tx, txq], ty,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n",
    "\n",
    "print(\"Saving model...\")\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"QA_final_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"QA_final_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# later...\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_from_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1caedaa1a3e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloaded_model_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_model_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# load weights into new model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predictor_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_from_json' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# load json and create model\n",
    "json_file = open('predictor_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"predictor_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "outputs = loaded_model.predict(test_question_data, batch_size = 1)\n",
    "print(\"type(outputs)\",type(outputs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def prepare_process_data(test_corpus, index):\n",
    "    final_data = []\n",
    "    question = test_corpus[index]['question']\n",
    "    docid = test_corpus[index]['docid']\n",
    "    processed_question = word_tokenize(question)\n",
    "    document = get_paragraph(docid,docs_corpus)\n",
    "    tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "    tfidf = get_okapibm25(tfs, total_docment,document)\n",
    "    top_1 = get_top_k_document(tfidf,question,1,document)\n",
    "    final_data.append(top_1)\n",
    "    print(\"final_data\",final_data)\n",
    "    return final_data\n",
    "\n",
    "count = 0\n",
    "predictions = []\n",
    "for index,answer_id in enumerate(outputs):\n",
    "    #print(index, answer_id)\n",
    "    story = prepare_process_data(test_for_predict,index)\n",
    "    doc_nouns = nlp(story) \n",
    "    id_list = []\n",
    "    for item in answer_id:\n",
    "        id_list.append(item)\n",
    "    index1 = np.argmax(id_list)\n",
    "        #print(\"type(number)\",type(number))\n",
    "        #print(\"number\",number)\n",
    "    text = idx2word[index1] \n",
    "    while text in stopwordsAll or text in punc or text == r\"``\" or text == \"'s\" or text == \"''\" or text == r\"_______\" or text == r\"____\" or text == r\"___\" or text == r\"'～\" or text == r\"“\":\n",
    "            del id_list[index1]\n",
    "            #new_answer_id = id_list[:index1] + id_list[index1+1:]\n",
    "            index1 = np.argmax(id_list)\n",
    "            text = idx2word[index1]\n",
    "    for entity in doc.ents:\n",
    "        if text in entity:\n",
    "            text = entity.text\n",
    "    for noun_chunk in doc_nouns.noun_chunks:\n",
    "        if text in noun_chunk.text:\n",
    "            text = noun_chunk.text\n",
    "    answer_str = str(index) + ',' + str(text)+'\\n'\n",
    "    predictions.append(answer_str)\n",
    "    count += 1\n",
    "print(\"count\",count)\n",
    "\n",
    "headers = ('id'+','+'answer'+'\\n')\n",
    "print(\"predictions[0]\",predictions[0])\n",
    "print(\"len(predictions)\",len(predictions))\n",
    "with open('output.txt', 'wt') as f:\n",
    "    f.write(headers)\n",
    "    for prediction in predictions:\n",
    "        f.write(prediction)\n",
    "    \n",
    "print(\"Finishing writing into output.txt!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
