{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shimei/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimei/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n",
      "/Users/shimei/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n",
      "/Users/shimei/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#firsr of all import all package and load the file\n",
    "import string\n",
    "from math import log\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords    \n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "jar = '/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/stanford-ner-2018-02-27/stanford-ner.jar'\n",
    "model = '/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/stanford-ner-2018-02-27/classifiers/english.conll.4class.distsim.crf.ser.gz'\n",
    "model1 = '/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/stanford-ner-2018-02-27/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "model2 = '/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/stanford-ner-2018-02-27/classifiers/english.muc.7class.distsim.crf.ser.gz'\n",
    "\n",
    "punc = string.punctuation\n",
    "stopwordsPart = set(stopwords.words('english'))\n",
    "stopwordsPart.remove('the')  \n",
    "stopwordsPart.remove('of') \n",
    "stopwordsAll = set(stopwords.words('english'))\n",
    "def opne_json(text):\n",
    "    with open(text,'r') as input_file:\n",
    "        document = json.load(input_file)\n",
    "    return document\n",
    "\n",
    "\n",
    "\n",
    "def get_tag_model(model,jar):\n",
    "    return StanfordNERTagger(model,jar)\n",
    "\n",
    "\n",
    "person_model = get_tag_model(model,jar)\n",
    "person_model2 = get_tag_model(model1,jar)\n",
    "number_model = get_tag_model(model2,jar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "documents_dict = opne_json(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/documents.json\")\n",
    "test_dict = opne_json(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/testing.json\")\n",
    "dev_dict = opne_json(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/devel.json\")\n",
    "train_dict = opne_json(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/training.json\")\n",
    "#query_lables = opne_json(\"QuestionLabel.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get paragraph from the training data\n",
    "def get_paragraph(docid,documents_dict):\n",
    "    #get the paragraph that contains the answer\n",
    "    for i in documents_dict:\n",
    "        if i['docid'] == docid:\n",
    "            document = i['text']\n",
    "            break\n",
    "    return document\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get TF \n",
    "def term_freqs(document):\n",
    "    tfs = defaultdict(dict)\n",
    "    tfs_forward = defaultdict(dict)\n",
    "    doc_id = 0\n",
    "    for sentence in document:\n",
    "        for token in word_tokenize(sentence):\n",
    "            if token not in stopwordsAll and token not in punc:  \n",
    "                term = lemmatizer.lemmatize(token.lower())\n",
    "                tfs[term][doc_id] = tfs[term].get(doc_id, 0) + 1 \n",
    "                tfs_forward[doc_id][term] = tfs[doc_id].get(term, 0) + 1 \n",
    "        doc_id += 1\n",
    "    return tfs,doc_id+1,tfs_forward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build TF_IDF model\n",
    "def get_tfidf(tfs, total_docment,tfs_forward):\n",
    "    document_length = {}\n",
    "    for doc_id,doc_list in tfs_forward.items():\n",
    "        length = 0\n",
    "        for term, freq in doc_list.items():\n",
    "            length += freq ** 2\n",
    "        length = length **0.5\n",
    "        document_length[doc_id] =  length\n",
    "    tfidf = defaultdict(dict)\n",
    "    for term, doc_list in tfs.items():\n",
    "        df = len(doc_list)\n",
    "        for doc_id, freq in doc_list.items(): \n",
    "            tfidf[term][doc_id] = (float(tfs[term][doc_id]) * log(total_docment / df))# / document_length[doc_id]\n",
    "    return tfidf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find top_k paragraph that may contain the answer\n",
    "def get_top_k_document(tfidf,query,k,document):\n",
    "    top_document_id = Counter()\n",
    "    for token in word_tokenize(query):\n",
    "        if token not in stopwordsAll:  \n",
    "             term = lemmatizer.lemmatize(token.lower())\n",
    "             term_tfidf = tfidf[term]\n",
    "             for docid, weight in term_tfidf.items():\n",
    "                 top_document_id[docid] += weight\n",
    "    top_document_id = top_document_id.most_common(k)\n",
    "    top_document = []\n",
    "    for document_id,weight in top_document_id:\n",
    "        top_document.append(document[document_id])\n",
    "    return top_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the key words in query\n",
    "def get_open_class_word(query):\n",
    "    tagged = nltk.pos_tag(word_tokenize(query), tagset=\"universal\")\n",
    "    return [p[0] for p in tagged if (p[1] in [\"NOUN\",\"VERB\"] or p[0].isdigit()) and p[0] not in [\"did\",\"do\",\"was\",\"were\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the NER with same tag\n",
    "def same_tag(ner_output):\n",
    "    word,tag = '',''\n",
    "    combo = []\n",
    "    for word1,tag1 in ner_output:\n",
    "        if tag1 == tag:\n",
    "            if word[-1] in ['(',')']:\n",
    "                word += word1\n",
    "            if word1 in [')']:\n",
    "                 word += word1\n",
    "            else:     \n",
    "                word += \" \" + word1\n",
    "        else:\n",
    "            combo.append((word,tag))\n",
    "            tag = tag1\n",
    "            word = word1\n",
    "            continue\n",
    "    if len(combo) != 0:\n",
    "        combo.pop(0)\n",
    "    return combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_in(key_words,sentence):\n",
    "    all_in = True\n",
    "    len1 = len(key_words)\n",
    "    word_in = 0\n",
    "    for i in key_words:\n",
    "        try:\n",
    "            index = sentence.index(i)\n",
    "            word_in += 1 \n",
    "        except ValueError:\n",
    "            continue\n",
    "    return len1 < 2*word_in\n",
    "\n",
    "def in_key_words(word,key_words):\n",
    "    in_key = False\n",
    "    for i in key_words:\n",
    "        if word.find(i) != -1:       \n",
    "            in_key = True\n",
    "            break\n",
    "    return in_key\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = get_paragraph(409,documents_dict)\n",
    "tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"Who performance in Anne of the Thousand Days garnered a Best Actor nod?\"\n",
    "top_k = get_top_k_document(tfidf,query,5,document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_list(query,top_k):\n",
    "    key_words = get_open_class_word(query)\n",
    "\n",
    "    answer_list = {}\n",
    "    answer_type = tag_answer_type(query)\n",
    "    if answer_type == \"O\":\n",
    "        return None\n",
    "    \n",
    "    for ans_sentence in top_k:\n",
    "        if most_in(key_words,ans_sentence) == False:\n",
    "            continue\n",
    "        word_list =  []\n",
    "        for word in word_tokenize(ans_sentence):\n",
    "            word_list.append(word)    \n",
    "        word_list_tag = number_model.tag(word_list)\n",
    "        word_list_tag = same_tag(word_list_tag)\n",
    "        for word,tags in word_list_tag:\n",
    "            if word not in answer_list.keys():\n",
    "                if word not in stopwordsAll and word not in punc and tags == answer_type and word not in key_words and in_key_words(word,key_words) == False:\n",
    "                    distance_list = []\n",
    "                    distance = 0\n",
    "                    for key_word in key_words:\n",
    "                        try:\n",
    "                            index = ans_sentence.index(key_word)\n",
    "                            distance_list.append(index)\n",
    "                        except ValueError:\n",
    "                            distance_list.append(5000)\n",
    "                    for index in distance_list:\n",
    "                        try:\n",
    "                            distance += abs(index - ans_sentence.index(word))\n",
    "                        except ValueError:\n",
    "                            print ('')\n",
    "                    answer_list[word] = distance\n",
    "    if  len(answer_list.items()) != 0:\n",
    "        return sorted(answer_list.items(), key=operator.itemgetter(1), reverse = True)[0][0].lower()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "richard burton\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "print (get_answer_list(query,top_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = {\n",
    "            'name':'PERSON',\n",
    "            'country': 'LOCATION',\n",
    "            'capital': 'LOCATION',\n",
    "            'newspaper':'ORGANIZATION',\n",
    "            'company':'ORGANIZATION',\n",
    "            'organization':'ORGANIZATION',\n",
    "            'person':'PERSON',\n",
    "            'cents':'MONEY',\n",
    "            'dollar':'MONEY',\n",
    "            'river':'LOCATION',\n",
    "            'location': 'LOCATION',\n",
    "            'mountain':'LOCATION',\n",
    "            'website':'ORGANIZATION',\n",
    "            'airline':'ORGANIZATION',\n",
    "            'percentage':'PERCENT',\n",
    "            'population':'NUMBER',\n",
    "            'decade':'DATE',\n",
    "            'where': 'LOCATION',\n",
    "            'when': 'DATE',           \n",
    "            'who': 'PERSON',     \n",
    "            'what scientist':'PERSON',\n",
    "            'what time':'TIME',\n",
    "            'what athlete':'PERSON',\n",
    "            'which athlete':'PERSON',\n",
    "            'what people': 'PERSON',\n",
    "            'what date':'DATE',\n",
    "            'what day':'DATE',\n",
    "            'what month':'DATE',\n",
    "            'what year': 'DATE',\n",
    "            'what era':'DATE',\n",
    "            'What age':'NUMBER',\n",
    "            'how old':'NUMBER',\n",
    "            'which anniversary':'DATE',\n",
    "            'what century':'DATE',\n",
    "            'time period':'NUMBER',\n",
    "            'what period':'NUMBER',\n",
    "            'what city' : 'LOCATION',\n",
    "            'which city' : 'LOCATION',\n",
    "            'what county':'LOCATION',\n",
    "            'what street':'LOCATION',\n",
    "            'what museum':'LOCATION',\n",
    "            'what region':'LOCATION',\n",
    "            'what road':'LOCATION',\n",
    "            'which publication':'ORGANIZATION',\n",
    "            'which council':'ORGANIZATION',\n",
    "            'what government':'ORGANIZATION',\n",
    "            'what college':'ORGANIZATION',\n",
    "            'which supporters' : 'PERSON',\n",
    "            'which footballer': 'PERSON',\n",
    "            'which actor':'PERSON',\n",
    "            'Which actress':'PERSON',\n",
    "            'which American actress':'PERSON',\n",
    "            'what activists':'PERSON',\n",
    "            'which team member' : 'PERSON',\n",
    "            'what football star': 'PERSON',\n",
    "            'which blogger': 'PERSON',\n",
    "            'which torchbearer':'PERSON',\n",
    "            'which wheelchair-bound torchbearer' : 'PERSON',\n",
    "            'how much': 'NUMBER',\n",
    "            'how many': 'NUMBER',\n",
    "            'how far':'NUMBER',\n",
    "            'how long':'NUMBER',\n",
    "            'how much of': 'PERCENT',\n",
    "            'How many of': 'PERCENT',\n",
    "            'by how much': 'PERCENT'        \n",
    "}\n",
    "\n",
    "money_list = ['cost', 'worth', 'spend', 'money', 'worth', 'invest']\n",
    "\n",
    "def tag_answer_type(question):\n",
    "    answer_type = 'O'\n",
    "    processed_question = []\n",
    "    processed_question_str = None\n",
    "    for token in [question]:\n",
    "        processed_question.append(token.lower())\n",
    "    processed_question_str = \" \".join(x for x in processed_question)\n",
    "    for k,v in rules.items():\n",
    "        if k in processed_question_str:\n",
    "            #print(k)\n",
    "            if k == 'how much':\n",
    "                for item in money_list:\n",
    "                    #print(\"item\", item)\n",
    "                    if item in processed_question_str:\n",
    "                        answer_type = 'MONEY'\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "                answer_type = rules.get(k, \"O\")    \n",
    "    return answer_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def get_accuracy(train_dict,documents_dict):\n",
    "    docidstart = train_dict[0]['docid']\n",
    "    document = get_paragraph(docidstart,documents_dict)\n",
    "    tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "    tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n",
    "    for i in train_dict:\n",
    "        docid = i['docid']\n",
    "        if docid != docidstart:\n",
    "            document = get_paragraph(docid,documents_dict)\n",
    "            tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "            tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n",
    "            docidstart = docid\n",
    "        query = i['question']\n",
    "        qaid = i['id']\n",
    "        top_k = get_top_k_document(tfidf,query,1,document)\n",
    "        potiential_answer = get_answer_list(query,top_k)\n",
    "        \n",
    "def output(test_dict,documents_dict):\n",
    "    output = open(\"out.txt\", \"w\")\n",
    "    docidstart = train_dict[0]['docid']\n",
    "    document = get_paragraph(docidstart,documents_dict)\n",
    "    tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "    tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n",
    "    for i in test_dict:\n",
    "        docid = i['docid']\n",
    "        if docid != docidstart:\n",
    "            document = get_paragraph(docid,documents_dict)\n",
    "            tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "            tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n",
    "            docidstart = docid\n",
    "        query = i['question']\n",
    "        qaid = i['id']\n",
    "        top_k = get_top_k_document(tfidf,query,1,document)\n",
    "        potiential_answer = get_answer_list(query,top_k)\n",
    "        print(potiential_answer)\n",
    "        print(\"qaid\",qaid)\n",
    "        print(\"potiential_answer\",potiential_answer)\n",
    "        output.write(str(qaid) + \",\" + str(potiential_answer) + '\\n')\n",
    "    \n",
    "#output(test_dict,documents_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "modelname = \"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/mymodel\"\n",
    "\n",
    "#train the model\n",
    "corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/documents.json\",'r') as docs:\n",
    "    data = json.load(docs)\n",
    "    for i in range(len(data)):\n",
    "        for para in data[i]['text']:\n",
    "            sents = sent_tokenize(para)\n",
    "            for sent in sents:\n",
    "                tokens = word_tokenize(sent)\n",
    "                corpus.append(tokens)\n",
    "    \n",
    "model = Word2Vec(corpus,min_count=1,hs=1,negative=0)\n",
    "model.save(modelname)\n",
    "\n",
    "#model = Word2Vec.load(modelname)\n",
    "#model.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3618\n",
      "line 0,None\n",
      "\n",
      "['0', 'None\\n']\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimei/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-194824aa5402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_k_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_question\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mPOS_top_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhat_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POS_top_k\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPOS_top_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mpoossible_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# word2vec for what type question\n",
    "import json\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "jar1 = '/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/stanford-postagger-2018-02-27/stanford-postagger.jar'\n",
    "model3 = '/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/stanford-postagger-2018-02-27/models/english-bidirectional-distsim.tagger'\n",
    "all_questions_dict = {}\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/testing.json\",'r') as test_file:\n",
    "    test_data = json.load(test_file)\n",
    "    for i in range(len(test_data)):\n",
    "        question = test_data[i]['question']\n",
    "        all_questions_dict[i] = question\n",
    "print(len(all_questions_dict))\n",
    " \n",
    "def get_POS_tag_model(model,jar):\n",
    "    return StanfordPOSTagger(model,jar)\n",
    "\n",
    "what_model = get_POS_tag_model(model3, jar1)\n",
    "\n",
    "possible_tag_list = [\"JJ\",\"JJR\",\"JJS\",\"NN\",\"NNS\",\"NNP\",\"NNPS\"]\n",
    "\n",
    "output = open(\"final.txt\", \"w\")\n",
    "with open('out.txt', 'rt') as f:\n",
    "    for line in f:\n",
    "        print(\"line\",line)\n",
    "        item = line.split(',')\n",
    "        print(item)\n",
    "        if item[1].startswith(\"None\"):\n",
    "            current_question = all_questions_dict.get(int(item[0]), 0)\n",
    "            top_k = get_top_k_document(tfidf,current_question,1,document)\n",
    "            print(top_k)\n",
    "            POS_top_k = what_model.tag_sents([[\" \".join(x) for x in [word_tokenize(top_k[0])]]])\n",
    "            print(\"POS_top_k\",POS_top_k)\n",
    "            poossible_answers = []\n",
    "            for tag in possible_tag_list:\n",
    "                for word_items in POS_top_k:\n",
    "                    for word_item in word_items:\n",
    "                        #print(\"word_item\",word_item)\n",
    "                        if word_item[1] == tag and word_item[0] != '(' and word_item[0] != ')':\n",
    "                            poossible_answers.append(word_item[0])\n",
    "            print(\"poossible_answers\",poossible_answers)\n",
    "            print(\"current_question\",current_question)\n",
    "            processed_question = []\n",
    "            que_list = word_tokenize(current_question)\n",
    "            for i in range(len(que_list)-1):\n",
    "                processed_question.append(que_list[i].lower())\n",
    "            print(\"processed_question\",processed_question)\n",
    "            print(len(processed_question)-1)\n",
    "            for i in range(len(processed_question)-1):\n",
    "                print(\"processed_question[i]\",processed_question[i])\n",
    "                if processed_question[i] == \"what\":\n",
    "                    for possible_answer in poossible_answers:\n",
    "                        score_dict = {}\n",
    "                        new_list = []\n",
    "                        processed_question[i] = possible_answer\n",
    "                        new_list = [\" \".join(x) for x in [processed_question]]\n",
    "                        print(\"new_list\",new_list)\n",
    "                        score = model.score(new_list)\n",
    "                        score_dict[possible_answer] = score[0]\n",
    "                        print(score_dict)\n",
    "                    answer = sorted(score_dict.items(), key=operator.itemgetter(1), reverse = True)[0][0].lower()\n",
    "                    print(\"answer\",answer)\n",
    "                    output.write(item[0] + \",\" + str(answer) + '\\n')\n",
    "        else:\n",
    "            output.write(line)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
