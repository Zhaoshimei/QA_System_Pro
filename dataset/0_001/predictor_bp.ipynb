{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import json\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent, LSTM, Dropout, Merge, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model \n",
    "\n",
    "# In[9]:\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from math import log\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, RepeatVector\n",
    "import gc\n",
    "\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "docs_corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/documents.json\",'r') as docs:\n",
    "    docs_corpus = json.load(docs)\n",
    "\n",
    "train_corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/training.json\",'r') as training:\n",
    "    train_corpus = json.load(training)\n",
    "print(\"len(train_corpus)\",len(train_corpus))\n",
    "\n",
    "test_for_train = train_corpus[:2000]\n",
    "\n",
    "dev_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/devel.json\",'r') as devel:\n",
    "    dev_corpus = json.load(devel)\n",
    "\n",
    "test_for_del = dev_corpus[:200]\n",
    "\n",
    "test_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/testing.json\",'r') as test:\n",
    "    test_corpus = json.load(test)\n",
    "    \n",
    "test_for_predict = test_corpus[:200]\n",
    "\n",
    "punc = string.punctuation\n",
    "stopwordsAll = set(stopwords.words('english'))\n",
    "\n",
    "def qestion_and_answer(question_id):\n",
    "    question = train_corpus[question_id]['question']\n",
    "    answer = train_corpus[question_id]['text']\n",
    "    processed_question = word_tokenize(question)\n",
    "    processed_answer = word_tokenize(answer)\n",
    "    para_id = train_corpus[question_id]['answer_paragraph']\n",
    "    docid = train_corpus[question_id]['docid']\n",
    "    # print(\"processed_question\",processed_question)\n",
    "    # print(\"answer\",answer)\n",
    "    # print(\"docid\",docid)\n",
    "    return processed_question, processed_answer, para_id,docid\n",
    "\n",
    "\n",
    "# qestion_and_answer(0)\n",
    "\n",
    "def doc_to_story(para_id,docid):\n",
    "    story = []\n",
    "\n",
    "    doc = docs_corpus[docid]\n",
    "    para = \"\"\n",
    "    for index, para_data in enumerate(doc['text']):\n",
    "        if index == para_id:\n",
    "            para = para_data\n",
    "        sents = sent_tokenize(para)\n",
    "        # print(\"sents\",sents)\n",
    "        for sent in sents:\n",
    "            tokens = word_tokenize(sent)\n",
    "            # print(\"tokens\",tokens)\n",
    "            story.append(tokens)\n",
    "    # print(\"story\",story)\n",
    "    return story\n",
    "\n",
    "\n",
    "# doc_to_story(0)\n",
    "\n",
    "# save [(story, question, answer)]\n",
    "def prepare_data(train_corpus):\n",
    "    final_data = []\n",
    "    print(len(train_corpus))\n",
    "    for i in range(len(train_corpus)):\n",
    "        processed_question, answer, para_id,docid = qestion_and_answer(i)\n",
    "        story = doc_to_story(para_id,docid)\n",
    "        final_data.append((story, processed_question, answer))\n",
    "        # print(\"final_data\",final_data)\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        story_list = []\n",
    "        for sent in story:\n",
    "            for w in sent:\n",
    "                story_list.append(w)\n",
    "        x = [word_idx[w] for w in story_list]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        for token in answer:\n",
    "            y[word_idx[token]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)\n",
    "\n",
    "\n",
    "def get_vocab(data,del_data):\n",
    "    vocab = set()\n",
    "    for story, q, answer in data:\n",
    "        list_words = []\n",
    "        for sent in story:\n",
    "            list_words = list_words + sent\n",
    "        vocab_list = list_words + q + answer\n",
    "        # print(\"vocab_list\",vocab_list)\n",
    "        vocab |= set(vocab_list)\n",
    "    for story, q, answer in del_data:\n",
    "        list_words = []\n",
    "        for sent in story:\n",
    "            list_words = list_words + sent\n",
    "        vocab_list = list_words + q + answer\n",
    "        # print(\"vocab_list\",vocab_list)\n",
    "        vocab |= set(vocab_list)\n",
    "    vocab = sorted(vocab)\n",
    "    return vocab\n",
    "\n",
    "data = prepare_data(test_for_train)\n",
    "print(\"len(data)\", len(data))\n",
    "\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word, 'n')\n",
    "    return lemma\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        story_list = []\n",
    "        for sent in story:\n",
    "            for w in sent:\n",
    "                story_list.append(w)\n",
    "        x = [word_idx[w] for w in story_list]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        for w in answer:\n",
    "            y[word_idx[w]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)\n",
    "\n",
    "\n",
    "def get_paragraph(docid, document_data):\n",
    "    # get the paragraph that contains the answer\n",
    "    for item in document_data:\n",
    "        if item['docid'] == docid:\n",
    "            document = item['text']\n",
    "            break\n",
    "    return document\n",
    "\n",
    "\n",
    "def term_freqs(document):\n",
    "    tfs = defaultdict(dict)\n",
    "    tfs_forward = defaultdict(dict)\n",
    "    doc_id = 0\n",
    "    for sentence in document:\n",
    "        for token in word_tokenize(sentence):\n",
    "            if token not in stopwordsAll and token not in punc:\n",
    "                term = lemmatize(token.lower())\n",
    "                tfs[term][doc_id] = tfs[term].get(doc_id, 0) + 1\n",
    "                tfs_forward[doc_id][term] = tfs[doc_id].get(term, 0) + 1\n",
    "        doc_id += 1\n",
    "    return tfs, doc_id + 1, tfs_forward\n",
    "\n",
    "\n",
    "def get_okapibm25(tf, total_docment, documents):\n",
    "    '''Calculate and return term weights based on okapibm25'''\n",
    "    k1, b, k3 = 1.5, 0.5, 0\n",
    "    okapibm25 = defaultdict(dict)\n",
    "\n",
    "    # calculate average doc length\n",
    "    total = 0\n",
    "    for d in documents:\n",
    "        total += len(d)\n",
    "    avg_doc_length = total / len(documents) * 1.0\n",
    "\n",
    "    for term, doc_list in tf.items():\n",
    "        df = len(doc_list)\n",
    "        for doc_id, freq in doc_list.items():\n",
    "            # term occurences in query\n",
    "            # qtf = question.count(term) # SEPCIAL\n",
    "            qtf = 1.2\n",
    "            idf = log((total_docment - df + 0.5) / df + 0.5)\n",
    "            tf_Dt = ((k1 + 1) * tf[term][doc_id]) / (\n",
    "            k1 * ((1 - b) + b * (len(documents[doc_id]) / avg_doc_length) + tf[term][doc_id]))\n",
    "            if qtf == 0:\n",
    "                third = 0\n",
    "            else:\n",
    "                third = ((k3 + 1) * qtf) / (k3 + qtf)\n",
    "                okapibm25[term][doc_id] = idf * tf_Dt * third\n",
    "\n",
    "    return okapibm25\n",
    "\n",
    "\n",
    "# find top_k paragraph that may contain the answer\n",
    "def get_top_k_document(tfidf, query, k, document):\n",
    "    top_document_id = Counter()\n",
    "    for token in word_tokenize(query):\n",
    "        if token not in stopwordsAll:\n",
    "            term = lemmatizer.lemmatize(token.lower())\n",
    "            term_tfidf = tfidf[term]\n",
    "            for docid, weight in term_tfidf.items():\n",
    "                top_document_id[docid] += weight\n",
    "    top_document_id = top_document_id.most_common(k)\n",
    "    top_document = []\n",
    "    for document_id, weight in top_document_id:\n",
    "        top_document.append(document_id)\n",
    "    return top_document\n",
    "\n",
    "def prepare_test_del(train_corpus):\n",
    "    final_data = []\n",
    "    print(len(train_corpus))\n",
    "    for i in range(len(train_corpus)):\n",
    "        story = []\n",
    "        question = train_corpus[i]['question']\n",
    "        processed_question, answer, para_id, docid = qestion_and_answer(i)\n",
    "        document = get_paragraph(docid, docs_corpus)\n",
    "        tfs, total_docment, tfs_forward = term_freqs(document)\n",
    "        tfidf = get_okapibm25(tfs, total_docment, document)\n",
    "        top_1 = get_top_k_document(tfidf, question, 1, document)\n",
    "        for item in top_1:\n",
    "            # print(\"item\", item)\n",
    "            story = doc_to_story(item, docid)\n",
    "            # print(\"story\",story)\n",
    "        final_data.append((story, processed_question, answer))\n",
    "        # print(\"final_data\",final_data)\n",
    "    return final_data\n",
    "\n",
    "def prepare_test_data(test_corpus):\n",
    "    final_data = []\n",
    "    for item in test_corpus:\n",
    "        question = item['question']\n",
    "        docid = item['docid']\n",
    "        processed_question = word_tokenize(question)\n",
    "        document = get_paragraph(docid,docs_corpus)\n",
    "        tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "        tfidf = get_okapibm25(tfs, total_docment,document)\n",
    "        top_1 = get_top_k_document(tfidf,question,1,document)\n",
    "        for item in top_1:\n",
    "            #print(\"top_1\", top_1)\n",
    "            story = doc_to_story(item, docid)\n",
    "            #print(\"story\",story)\n",
    "        final_data.append((story, processed_question))\n",
    "        #print(\"final_data\",final_data)\n",
    "    return \n",
    "\n",
    "del_data = prepare_test_del(test_for_del)\n",
    "print(\"len(del_data)\", len(del_data))\n",
    "\n",
    "def vectorize_stories_test(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    for story, query in data:\n",
    "        story_list = []\n",
    "        for sent in story:\n",
    "            for w in sent:\n",
    "                story_list.append(w)\n",
    "        x = [word_idx[w] for w in story_list]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen)\n",
    "\n",
    "test_data = prepare_test_data(test_for_predict)\n",
    "print(\"len(test_data)\",len(test_data))\n",
    "\n",
    "\n",
    "\n",
    "vocab = get_vocab(data,del_data)\n",
    "# print(\"vocab\",vocab)\n",
    "print(\"len(vocab)\", len(vocab))\n",
    "\n",
    "\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "WORD2VEC_EMBED_SIZE = 100\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "\n",
    "WORD2VEC_EMBED_SIZE = 100\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word2idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "idx2word = dict((i + 1, c) for i, c in enumerate(vocab))\n",
    "print(\"len(word2idx)\",len(word2idx))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in data)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in data)))\n",
    "\n",
    "x, xq, y = vectorize_stories(data, word2idx, story_maxlen, query_maxlen)\n",
    "tx, txq, ty = vectorize_stories(del_data, word2idx, story_maxlen, query_maxlen)\n",
    "\n",
    "print(\"Prepare test data...\")\n",
    "\n",
    "test_story, test_question = vectorize_stories_test(test_data, word2idx, story_maxlen, query_maxlen)\n",
    "\n",
    "test_question_data = [test_story, test_question]\n",
    "\n",
    "del data\n",
    "del del_data\n",
    "gc.collect()\n",
    "\n",
    "#word2vec\n",
    "modelname = \"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/mywvmodel.model\"\n",
    "model = Word2Vec.load(modelname)  # 3个文件放在一起：Word60.model   Word60.model.syn0.npy   Word60.model.syn1neg.npy\n",
    "print(\"read model successful\")\n",
    "embedding_weights = np.zeros((vocab_size, WORD2VEC_EMBED_SIZE))\n",
    "for word, index in word2idx.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = model[word.lower()]\n",
    "    except KeyError:\n",
    "        pass  # keep as zero (not ideal, but what else can we do?)\n",
    "\n",
    "del model\n",
    "del word2idx\n",
    "gc.collect()\n",
    "\n",
    "#former\n",
    "\n",
    "# print('vocab = {}'.format(vocab))\n",
    "print('x.shape = {}'.format(x.shape))\n",
    "print('xq.shape = {}'.format(xq.shape))\n",
    "print('y.shape = {}'.format(y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "qenc = Sequential()\n",
    "qenc.add(Embedding(output_dim=WORD2VEC_EMBED_SIZE, input_dim=vocab_size,\n",
    "                   weights=[embedding_weights], mask_zero=True))\n",
    "qenc.add(LSTM(EMBED_HIDDEN_SIZE, input_shape=(None,story_maxlen), return_sequences=False))\n",
    "qenc.add(Dropout(0.3))\n",
    "\n",
    "aenc = Sequential()\n",
    "aenc.add(Embedding(output_dim=WORD2VEC_EMBED_SIZE, input_dim=vocab_size,\n",
    "                   weights=[embedding_weights], mask_zero=True))\n",
    "aenc.add(LSTM(EMBED_HIDDEN_SIZE, input_shape=(None,query_maxlen), return_sequences=False))\n",
    "#aenc.add(RepeatVector(story_maxlen))\n",
    "aenc.add(Dropout(0.3))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([qenc, aenc], mode=\"sum\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "print('Training')\n",
    "\n",
    "\n",
    "#MODEL_DIR = \"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/my_model.h5\"\n",
    "#model.save(MODEL_DIR)\n",
    "\n",
    "model.fit([x, xq], y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.05)\n",
    "loss, acc = model.evaluate([tx, txq], ty,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n",
    "\n",
    "print('Testing...')\n",
    "\n",
    "#from keras.models import load_model  \n",
    "#my_model = load_model(MODEL_DIR) \n",
    "outputs = model.predict(test_question_data, batch_size = 1)\n",
    "print(\"type(outputs)\",type(outputs))\n",
    "\n",
    "count = 0\n",
    "predictions = []\n",
    "for index,answer_id in enumerate(outputs):\n",
    "    print(index, answer_id)\n",
    "    id_list = []\n",
    "    for i,n in enumerate(answer_id):\n",
    "        if n == '1':\n",
    "            id_list.append(i)\n",
    "    text = \" \".join(idx2word[i] for i in id_list)\n",
    "    answer_str = str(index) + ',' + str(text)\n",
    "    predictions.append(answer_str)\n",
    "    count += 1\n",
    "print(\"count\",count)\n",
    "\n",
    "headers = ('id'+','+'answer')\n",
    "print(\"predictions[0]\",predictions[0])\n",
    "print(\"len(predictions)\",len(predictions))\n",
    "with open('output.txt', 'wt') as f:\n",
    "    f.write(headers)\n",
    "    for prediction in predictions:\n",
    "        f.write(prediction)\n",
    "    \n",
    "print(\"Finishing writing into output.txt!\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
