{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\nltk\\tag\\stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n",
      "G:\\Anaconda\\lib\\site-packages\\nltk\\tag\\stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n",
      "G:\\Anaconda\\lib\\site-packages\\nltk\\tag\\stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#firsr of all import all package and load the file\n",
    "import string\n",
    "from math import log\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords    \n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "jar = 'stanford-ner.jar'\n",
    "model = 'english.conll.4class.distsim.crf.ser.gz'\n",
    "model1 = 'english.all.3class.distsim.crf.ser.gz'\n",
    "model2 = 'english.muc.7class.distsim.crf.ser.gz'\n",
    "punc = string.punctuation\n",
    "stopwordsPart = set(stopwords.words('english'))\n",
    "stopwordsPart.remove('the')  \n",
    "stopwordsPart.remove('of') \n",
    "stopwordsAll = set(stopwords.words('english'))\n",
    "def opne_json(text):\n",
    "    with open(text,'r') as input_file:\n",
    "        document = json.load(input_file)\n",
    "    return document\n",
    "\n",
    "\n",
    "\n",
    "def get_tag_model(model,jar):\n",
    "    return StanfordNERTagger(model,jar)\n",
    "\n",
    "person_model = get_tag_model(model,jar)\n",
    "person_model2 = get_tag_model(model1,jar)\n",
    "number_model = get_tag_model(model2,jar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "documents_dict = opne_json(\"documents.json\")\n",
    "test_dict = opne_json(\"testing.json\")\n",
    "dev_dict = opne_json(\"devel.json\")\n",
    "train_dict = opne_json(\"training.json\")\n",
    "query_lables = opne_json(\"QuestionLabel.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get paragraph from the training data\n",
    "def get_paragraph(docid,documents_dict):\n",
    "    #get the paragraph that contains the answer\n",
    "    for i in documents_dict:\n",
    "        if i['docid'] == docid:\n",
    "            document = i['text']\n",
    "            break\n",
    "    return document\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get TF \n",
    "def term_freqs(document):\n",
    "    tfs = defaultdict(dict)\n",
    "    tfs_forward = defaultdict(dict)\n",
    "    doc_id = 0\n",
    "    for sentence in document:\n",
    "        for token in word_tokenize(sentence):\n",
    "            if token not in stopwordsAll and token not in punc:  \n",
    "                term = lemmatizer.lemmatize(token.lower())\n",
    "                tfs[term][doc_id] = tfs[term].get(doc_id, 0) + 1 \n",
    "                tfs_forward[doc_id][term] = tfs[doc_id].get(term, 0) + 1 \n",
    "        doc_id += 1\n",
    "    return tfs,doc_id+1,tfs_forward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build TF_IDF model\n",
    "def get_tfidf(tfs, total_docment,tfs_forward):\n",
    "    document_length = {}\n",
    "    for doc_id,doc_list in tfs_forward.items():\n",
    "        length = 0\n",
    "        for term, freq in doc_list.items():\n",
    "            length += freq ** 2\n",
    "        length = length **0.5\n",
    "        document_length[doc_id] =  length\n",
    "    tfidf = defaultdict(dict)\n",
    "    for term, doc_list in tfs.items():\n",
    "        df = len(doc_list)\n",
    "        for doc_id, freq in doc_list.items(): \n",
    "            tfidf[term][doc_id] = (float(tfs[term][doc_id]) * log(total_docment / df))# / document_length[doc_id]\n",
    "    return tfidf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find top_k paragraph that may contain the answer\n",
    "def get_top_k_document(tfidf,query,k,document):\n",
    "    top_document_id = Counter()\n",
    "    for token in word_tokenize(query):\n",
    "        if token not in stopwordsAll:  \n",
    "             term = lemmatizer.lemmatize(token.lower())\n",
    "             term_tfidf = tfidf[term]\n",
    "             for docid, weight in term_tfidf.items():\n",
    "                 top_document_id[docid] += weight\n",
    "    top_document_id = top_document_id.most_common(k)\n",
    "    top_document = []\n",
    "    for document_id,weight in top_document_id:\n",
    "        top_document.append(document[document_id])\n",
    "    return top_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the key words in query\n",
    "def get_open_class_word(query):\n",
    "    query = nltk.word_tokenize(query)\n",
    "    tagged = nltk.pos_tag(query)#nltk.word_tokenize(query))#, tagset=\"universal\")\n",
    "    #return [p[0] for p in tagged if (p[1] in [\"NOUN\",\"VERB\"] or p[0].isdigit()) and p[0] not in [\"did\",\"do\",\"was\",\"were\"ï¼Œ\"is\",\"are\"]]\n",
    "    return [p[0] for p in tagged if (p[1] in [\"NN\",\"NNP\",\"VB\",\"VBD\"] or p[0].isdigit()) and p[0] not in [\"did\",\"do\",\"was\",\"were\",\"is\",\"are\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the NER with same tag\n",
    "def same_tag(ner_output):\n",
    "    word,tag = 'the','START'\n",
    "    combo = []\n",
    "    for word1,tag1 in ner_output:\n",
    "        '''\n",
    "        if tag1 == \"O\" and word1 not in stopwordsAll and word1 not in punc:\n",
    "            combo.append((word,tag))\n",
    "            tag = tag1\n",
    "            word = word1\n",
    "            continue\n",
    "        '''\n",
    "        if tag1 == tag:\n",
    "            if word[-1] in ['(',')']:\n",
    "                word += word1\n",
    "            if word1 in [')']:\n",
    "                 word += word1\n",
    "            else:     \n",
    "                word += \" \" + word1\n",
    "        else:\n",
    "            combo.append((word,tag))\n",
    "            tag = tag1\n",
    "            word = word1\n",
    "            continue\n",
    "    if len(combo) != 0:\n",
    "        combo.pop(0)\n",
    "    return combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_in(key_words,sentence):\n",
    "    all_in = True\n",
    "    len1 = len(key_words)\n",
    "    word_in = 0\n",
    "    for i in key_words:\n",
    "        try:\n",
    "            index = sentence.index(i)\n",
    "            word_in += 1 \n",
    "        except ValueError:\n",
    "            continue\n",
    "    return len1 < 2*word_in\n",
    "\n",
    "def in_key_words(word,key_words):\n",
    "    in_key = False\n",
    "    for i in key_words:\n",
    "        if word.find(i) != -1:       \n",
    "            in_key = True\n",
    "            break\n",
    "    return in_key\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = get_paragraph(409,documents_dict)\n",
    "tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"Who performance in Anne of the Thousand Days garnered a Best Actor nod?\"\n",
    "top_k = get_top_k_document(tfidf,query,5,document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = {\n",
    "            'name':'PERSON',\n",
    "            'country': 'LOCATION',\n",
    "            'capital': 'LOCATION',\n",
    "            'newspaper':'ORGANIZATION',\n",
    "            'company':'ORGANIZATION',\n",
    "            'city': 'LOCATION',\n",
    "            'person':'PERSON',\n",
    "            'location': 'LOCATION',\n",
    "            'mountain':'LOCATION',\n",
    "            'website':'ORGANIZATION',\n",
    "            'airline':'ORGANIZATION',\n",
    "            'which organization': 'ORGANIZATION',\n",
    "            'where': 'LOCATION',\n",
    "            'when': 'DATE',           \n",
    "            'who': 'PERSON',     \n",
    "            'what scientist':'PERSON',\n",
    "            'what time':'TIME',\n",
    "            'what athlete':'PERSON',\n",
    "            'which athlete':'PERSON',\n",
    "            'what people': 'PERSON',\n",
    "            'what date':'DATE',\n",
    "            'what day':'DATE',\n",
    "            'what year': 'DATE',\n",
    "            'what city' : 'LOCATION',\n",
    "            'which company': 'ORGANIZATION' ,\n",
    "            'which publication':'ORGANIZATION',\n",
    "            'what government':'ORGANIZATION',\n",
    "            'which supporters' : 'PERSON',\n",
    "            'which footballer': 'PERSON',\n",
    "            'which actor':'PERSON',\n",
    "            'Which actress':'PERSON',\n",
    "            'which American actress':'PERSON',\n",
    "            'what activists':'PERSON',\n",
    "            'which team member' : 'PERSON',\n",
    "            'what football star': 'PERSON',\n",
    "            'which blogger': 'PERSON',\n",
    "            'which torchbearer':'PERSON',\n",
    "            'which wheelchair-bound torchbearer' : 'PERSON',\n",
    "            'how much of': 'PERCENT',\n",
    "            'by how much': 'PERCENT',\n",
    "            'how much': 'O'\n",
    "            \n",
    "            \n",
    "}\n",
    "\n",
    "money_list = ['cost', 'worth', 'spend', 'money', 'worth', 'invest']\n",
    "\n",
    "def tag_answer_type(question):\n",
    "    answer_type = 'O'\n",
    "    processed_question = []\n",
    "    processed_question_str = None\n",
    "    for token in [question]:\n",
    "        processed_question.append(token.lower())\n",
    "    processed_question_str = \" \".join(x for x in processed_question)\n",
    "    for k,v in rules.items():\n",
    "        if k in processed_question_str:\n",
    "            #print(k)\n",
    "            if k == 'how much':\n",
    "                for item in money_list:\n",
    "                    #print(\"item\", item)\n",
    "                    if item in processed_question_str:\n",
    "                        answer_type = 'MONEY'\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "                answer_type = rules.get(k, \"O\")    \n",
    "    return answer_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_list(query,top_k):\n",
    "    key_words = get_open_class_word(query)\n",
    "    answer_list = {}\n",
    "    answer_type = tag_answer_type(query)\n",
    "    for ans_sentence in top_k:\n",
    "        #if most_in(key_words,ans_sentence) == False:\n",
    "             #continue\n",
    "        if answer_type == \"O\":\n",
    "            word_list = nltk.word_tokenize(ans_sentence)\n",
    "            word_list_tag = nltk.pos_tag(word_list)\n",
    "            word_list_tag = same_tag(word_list_tag)\n",
    "            answer_type = [\"NN\",\"NNP\"]\n",
    "            \n",
    "        else:\n",
    "            word_list =  []\n",
    "            for word in word_tokenize(ans_sentence):\n",
    "                word_list.append(word)    \n",
    "            word_list_tag = number_model.tag(word_list)\n",
    "            word_list_tag = same_tag(word_list_tag)\n",
    "            answer_type = [answer_type]\n",
    "            \n",
    "        for word,tags in word_list_tag:\n",
    "            if word not in answer_list.keys():\n",
    "                if word not in stopwordsAll and word not in punc and tags in answer_type and word not in key_words and in_key_words(word,key_words) == False:\n",
    "                    distance_list = []\n",
    "                    distance = 0\n",
    "                    for key_word in key_words:\n",
    "                        try:\n",
    "                            index = ans_sentence.index(key_word)\n",
    "                            distance_list.append(index)\n",
    "                        except ValueError:\n",
    "                            distance_list.append(5000)\n",
    "                    for index in distance_list:\n",
    "                        try:\n",
    "                            distance += abs(index - ans_sentence.index(word))\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                    answer_list[word] = distance\n",
    "                    \n",
    "    if  len(answer_list.items()) != 0:\n",
    "        return sorted(answer_list.items(), lambda x, y: cmp(x[1], y[1]))[0][0].lower()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Richard Burton', 14415), (u'Genevi\\xe8ve Bujold)', 14477), (u'Anthony Quayle', 14569), (u'Mary', 14715), (u'John Wayne', 14949), (u'Maxwell Anderson', 15047), (u'Katharine Hepburn', 15093), (u'Wallis', 16287), (u'Hal B. Wallis', 16343)]\n"
     ]
    }
   ],
   "source": [
    "print get_answer_list(query,top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016\n",
      "mid-2000s\n",
      "6th century\n",
      "6th century\n",
      "carrigaline\n",
      "bus Ã©ireann\n",
      "city\n",
      "north cathedral\n",
      "company\n",
      "two\n",
      "muskerry east\n",
      "black rent\n",
      "prince john\n",
      "prince john\n",
      "1874\n",
      "1874\n",
      "huguenot\n",
      "about 2,100\n",
      "None\n",
      "1840\n",
      "george francis train\n",
      "george francis train\n",
      "city centre\n",
      "hurling and football\n",
      "2016\n",
      "2016\n",
      "cork\n",
      "cork\n",
      "settlement\n",
      "foggy\n",
      "None\n",
      "popes quay\n",
      "town\n",
      "black death\n",
      "jack lynch\n",
      "local government\n",
      "business innovation hub\n",
      "57\n",
      "kerry\n",
      "met Ã©ireann\n",
      "county hall\n",
      "the elysian\n",
      "1980s\n",
      "1980s\n",
      "sport\n",
      "five\n",
      "city\n",
      "two\n",
      "echo\n",
      "cork examiner\n",
      "gaelic\n",
      "30\n",
      "lady\n",
      "atkins hall\n",
      "city\n",
      "st mary 's cathedral\n",
      "knowledge\n",
      "the rubicon centre\n",
      "ucc\n",
      "ucc 98.3fm\n",
      "None\n",
      "fitzgerald 's park\n",
      "None\n",
      "rhoticity\n",
      "institute for choreography and dance\n",
      "cillian murphy\n",
      "citation\n",
      "2005\n",
      "None\n",
      "1952\n",
      "counterweight\n",
      "the ussr\n",
      "communism\n",
      "socialism and communism\n",
      "romania\n",
      "hungary\n",
      "side\n",
      "1954\n",
      "invasion\n",
      "1968\n",
      "success\n",
      "1989\n",
      "us\n",
      "albania\n",
      "police force\n",
      "burglar\n",
      "balance\n",
      "czechoslovakia\n",
      "hungary\n",
      "hungary\n",
      "james dunn\n",
      "molotov\n",
      "order\n",
      "containment\n",
      "None\n",
      "the ussr\n",
      "member\n",
      "seven\n",
      "march 1952\n",
      "1954\n",
      "50\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def get_accuracy(train_dict,documents_dict):\n",
    "    docidstart = train_dict[0]['docid']\n",
    "    document = get_paragraph(docidstart,documents_dict)\n",
    "    tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "    tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n",
    "    right,total = 0,0\n",
    "    for i in train_dict[7000:]:\n",
    "        docid = i['docid']\n",
    "        if docid != docidstart:\n",
    "            document = get_paragraph(docid,documents_dict)\n",
    "            tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "            tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n",
    "            docidstart = docid\n",
    "        query = i['question']\n",
    "        #qaid = i['id']\n",
    "        answer = i['text']\n",
    "        top_k = get_top_k_document(tfidf,query,1,document)\n",
    "        potiential_answer = get_answer_list(query,top_k)\n",
    "        print potiential_answer\n",
    "        print answer\n",
    "        if answer == potiential_answer:\n",
    "            right += 1\n",
    "        total += 1\n",
    "        if total == 50:\n",
    "            print total\n",
    "            print right\n",
    "            break\n",
    "        \n",
    "            \n",
    "        \n",
    "def output(test_dict,documents_dict):\n",
    "    output = open(\"out.txt\", \"w\")\n",
    "    docidstart = train_dict[0]['docid']\n",
    "    document = get_paragraph(docidstart,documents_dict)\n",
    "    tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "    tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n",
    "    for i in test_dict:\n",
    "        docid = i['docid']\n",
    "        if docid != docidstart:\n",
    "            document = get_paragraph(docid,documents_dict)\n",
    "            tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "            tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n",
    "            docidstart = docid\n",
    "        query = i['question']\n",
    "        qaid = i['id']\n",
    "        top_k = get_top_k_document(tfidf,query,1,document)\n",
    "        potiential_answer = get_answer_list(query,top_k)\n",
    "        try:\n",
    "            output.write(str(qaid) + \",\" +str(potiential_answer) + '\\n')\n",
    "        except UnicodeEncodeError:\n",
    "            output.write(str(qaid) + '\\n')\n",
    "            print potiential_answer\n",
    "            \n",
    "def output_csv(test_dict,documents_dict):\n",
    "    csv_file = open('outpur.csv', 'w')\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['id', 'answer'])\n",
    "    docidstart = train_dict[0]['docid']\n",
    "    document = get_paragraph(docidstart,documents_dict)\n",
    "    tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "    tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n",
    "    for i in test_dict:\n",
    "        docid = i['docid']\n",
    "        if docid != docidstart:\n",
    "            document = get_paragraph(docid,documents_dict)\n",
    "            tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "            tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n",
    "            docidstart = docid\n",
    "        query = i['question']\n",
    "        qaid = i['id']\n",
    "        top_k = get_top_k_document(tfidf,query,1,document)\n",
    "        potiential_answer = get_answer_list(query,top_k)\n",
    "        print potiential_answer\n",
    "        writer.writerow([qaid, potiential_answer])\n",
    "\n",
    "#output_csv(test_dict,documents_dict)\n",
    "get_accuracy(train_dict,documents_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
