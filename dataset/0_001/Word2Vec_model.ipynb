{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(corpus1) 93515\n",
      "corpus1[0] ['First', 'recognized', 'in', '1900', 'by', 'Max', 'Planck', ',', 'it', 'was', 'originally', 'the', 'proportionality', 'constant', 'between', 'the', 'minimal', 'increment', 'of', 'energy', ',', 'E', ',', 'of', 'a', 'hypothetical', 'electrically', 'charged', 'oscillator', 'in', 'a', 'cavity', 'that', 'contained', 'black', 'body', 'radiation', ',', 'and', 'the', 'frequency', ',', 'f', ',', 'of', 'its', 'associated', 'electromagnetic', 'wave', '.']\n",
      "len(corpus2) 86758\n",
      "corpus2[0] ['A', 'kilogram', 'could', 'be', 'definined', 'as', 'having', 'a', 'Planck', 'constant', 'of', 'what', 'value', '?']\n",
      "len(corpus3) 3618\n",
      "corpus3[0] ['Modern', 'browser', 'support', 'standards-based', 'and', 'defacto', 'what', '?']\n",
      "183891\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "modelname = \"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/mywvmodel.model\"\n",
    "\n",
    "\n",
    "#三个\n",
    "corpus1 = []\n",
    "docs_corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/documents.json\",\n",
    "          'r') as docs:\n",
    "    docs_corpus = json.load(docs)\n",
    "    for i in range(len(docs_corpus)):\n",
    "        for para in docs_corpus[i]['text']:\n",
    "            sents = sent_tokenize(para)\n",
    "            # print(\"sents\",sents)\n",
    "            for sent in sents:\n",
    "                tokens = word_tokenize(sent)\n",
    "                # print(\"tokens\",tokens)\n",
    "                corpus1.append(tokens)\n",
    "print(\"len(corpus1)\", len(corpus1))\n",
    "print(\"corpus1[0]\", corpus1[0])\n",
    "\n",
    "corpus2 = []\n",
    "train_corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/training.json\",\n",
    "          'r') as training:\n",
    "    train_corpus = json.load(training)\n",
    "    # print(\"len(train_corpus)\",len(train_corpus))\n",
    "    for i in range(len(train_corpus)):\n",
    "        question = train_corpus[i]['question']\n",
    "        answer = train_corpus[i]['text']\n",
    "        processed_question = word_tokenize(question)\n",
    "        corpus2.append(processed_question)\n",
    "        processed_answer = word_tokenize(answer)\n",
    "        corpus2.append(processed_answer)\n",
    "print(\"len(corpus2)\", len(corpus2))\n",
    "print(\"corpus2[0]\", corpus2[0])\n",
    "\n",
    "# test_for_train = train_corpus[:10000]\n",
    "\n",
    "dev_corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/devel.json\", 'r') as devel:\n",
    "    dev_corpus = json.load(devel)\n",
    "\n",
    "# test_for_del = dev_corpus[:1000]\n",
    "\n",
    "corpus3 = []\n",
    "test_corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/testing.json\", 'r') as test:\n",
    "    test_corpus = json.load(test)\n",
    "    for i in range(len(test_corpus)):\n",
    "        question = test_corpus[i]['question']\n",
    "        processed_question = word_tokenize(question)\n",
    "        corpus3.append(processed_question)\n",
    "print(\"len(corpus3)\", len(corpus3))\n",
    "print(\"corpus3[0]\", corpus3[0])\n",
    "\n",
    "print(len(corpus1)+len(corpus2)+len(corpus3))\n",
    "model = Word2Vec(corpus1+corpus2+corpus3,min_count=1)\n",
    "model.save(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
