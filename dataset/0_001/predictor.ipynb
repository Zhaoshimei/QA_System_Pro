{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_corpus) 43379\n",
      "10000\n",
      "len(data) 10000\n",
      "200\n",
      "len(del_data) 200\n",
      "len(test_for_predict) 10\n",
      "len(test_data) 10\n",
      "len(vocab) 42516\n",
      "RNN / Embed / Sent / Query = <class 'keras.layers.recurrent.LSTM'>, 50, 100, 100\n",
      "len(word2idx) 42516\n",
      "Prepare test data...\n",
      "read model successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimei/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:372: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (10000, 1305)\n",
      "xq.shape = (10000, 60)\n",
      "y.shape = (10000, 42517)\n",
      "story_maxlen, query_maxlen = 1305, 60\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import json\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent, LSTM, Dropout, Merge, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model \n",
    "from keras.models import model_from_json\n",
    "\n",
    "# In[9]:\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from math import log\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, RepeatVector\n",
    "import gc\n",
    "\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "docs_corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/documents.json\",'r') as docs:\n",
    "    docs_corpus = json.load(docs)\n",
    "\n",
    "train_corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/training.json\",'r') as training:\n",
    "    train_corpus = json.load(training)\n",
    "print(\"len(train_corpus)\",len(train_corpus))\n",
    "\n",
    "test_for_train = train_corpus[:10000]\n",
    "\n",
    "dev_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/devel.json\",'r') as devel:\n",
    "    dev_corpus = json.load(devel)\n",
    "\n",
    "test_for_del = dev_corpus[:200]\n",
    "\n",
    "test_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/testing.json\",'r') as test:\n",
    "    test_corpus = json.load(test)\n",
    "    \n",
    "test_for_predict = test_corpus[:10]\n",
    "\n",
    "punc = string.punctuation\n",
    "stopwordsAll = set(stopwords.words('english'))\n",
    "\n",
    "def qestion_and_answer(question_id):\n",
    "    question = train_corpus[question_id]['question']\n",
    "    answer = train_corpus[question_id]['text']\n",
    "    processed_question = word_tokenize(question)\n",
    "    processed_answer = word_tokenize(answer)\n",
    "    para_id = train_corpus[question_id]['answer_paragraph']\n",
    "    docid = train_corpus[question_id]['docid']\n",
    "    # print(\"processed_question\",processed_question)\n",
    "    # print(\"answer\",answer)\n",
    "    # print(\"docid\",docid)\n",
    "    return processed_question, processed_answer, para_id,docid\n",
    "\n",
    "\n",
    "# qestion_and_answer(0)\n",
    "\n",
    "def doc_to_story(para_id,docid):\n",
    "    story = []\n",
    "\n",
    "    doc = docs_corpus[docid]\n",
    "    para = \"\"\n",
    "    for index, para_data in enumerate(doc['text']):\n",
    "        if index == para_id:\n",
    "            para = para_data\n",
    "        sents = sent_tokenize(para)\n",
    "        # print(\"sents\",sents)\n",
    "        for sent in sents:\n",
    "            tokens = word_tokenize(sent)\n",
    "            # print(\"tokens\",tokens)\n",
    "            story.append(tokens)\n",
    "    # print(\"story\",story)\n",
    "    return story\n",
    "\n",
    "\n",
    "# doc_to_story(0)\n",
    "\n",
    "# save [(story, question, answer)]\n",
    "def prepare_data(train_corpus):\n",
    "    final_data = []\n",
    "    print(len(train_corpus))\n",
    "    for i in range(len(train_corpus)):\n",
    "        processed_question, answer, para_id,docid = qestion_and_answer(i)\n",
    "        story = doc_to_story(para_id,docid)\n",
    "        final_data.append((story, processed_question, answer))\n",
    "        # print(\"final_data\",final_data)\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        story_list = []\n",
    "        for sent in story:\n",
    "            for w in sent:\n",
    "                story_list.append(w)\n",
    "        x = [word_idx[w] for w in story_list]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        for token in answer:\n",
    "            y[word_idx[token]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)\n",
    "\n",
    "\n",
    "def get_vocab(data,del_data,test_data):\n",
    "    vocab = set()\n",
    "    for story, q, answer in data:\n",
    "        list_words = []\n",
    "        for sent in story:\n",
    "            list_words = list_words + sent\n",
    "        vocab_list = list_words + q + answer\n",
    "        # print(\"vocab_list\",vocab_list)\n",
    "        vocab |= set(vocab_list)\n",
    "    for story, q, answer in del_data:\n",
    "        list_words = []\n",
    "        for sent in story:\n",
    "            list_words = list_words + sent\n",
    "        vocab_list = list_words + q + answer\n",
    "        # print(\"vocab_list\",vocab_list)\n",
    "        vocab |= set(vocab_list)\n",
    "    for story, q in test_data:\n",
    "        list_words = []\n",
    "        for sent in story:\n",
    "            list_words = list_words + sent\n",
    "        vocab_list = list_words + q\n",
    "        # print(\"vocab_list\",vocab_list)\n",
    "        vocab |= set(vocab_list)\n",
    "    vocab = sorted(vocab)\n",
    "    return vocab\n",
    "\n",
    "data = prepare_data(test_for_train)\n",
    "print(\"len(data)\", len(data))\n",
    "\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word, 'n')\n",
    "    return lemma\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        story_list = []\n",
    "        for sent in story:\n",
    "            for w in sent:\n",
    "                story_list.append(w)\n",
    "        x = [word_idx[w] for w in story_list]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        for w in answer:\n",
    "            y[word_idx[w]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)\n",
    "\n",
    "\n",
    "def get_paragraph(docid, document_data):\n",
    "    # get the paragraph that contains the answer\n",
    "    for item in document_data:\n",
    "        if item['docid'] == docid:\n",
    "            document = item['text']\n",
    "            break\n",
    "    return document\n",
    "\n",
    "\n",
    "def term_freqs(document):\n",
    "    tfs = defaultdict(dict)\n",
    "    tfs_forward = defaultdict(dict)\n",
    "    doc_id = 0\n",
    "    for sentence in document:\n",
    "        for token in word_tokenize(sentence):\n",
    "            if token not in stopwordsAll and token not in punc:\n",
    "                term = lemmatize(token.lower())\n",
    "                tfs[term][doc_id] = tfs[term].get(doc_id, 0) + 1\n",
    "                tfs_forward[doc_id][term] = tfs[doc_id].get(term, 0) + 1\n",
    "        doc_id += 1\n",
    "    return tfs, doc_id + 1, tfs_forward\n",
    "\n",
    "\n",
    "def get_okapibm25(tf, total_docment, documents):\n",
    "    '''Calculate and return term weights based on okapibm25'''\n",
    "    k1, b, k3 = 1.5, 0.5, 0\n",
    "    okapibm25 = defaultdict(dict)\n",
    "\n",
    "    # calculate average doc length\n",
    "    total = 0\n",
    "    for d in documents:\n",
    "        total += len(d)\n",
    "    avg_doc_length = total / len(documents) * 1.0\n",
    "\n",
    "    for term, doc_list in tf.items():\n",
    "        df = len(doc_list)\n",
    "        for doc_id, freq in doc_list.items():\n",
    "            # term occurences in query\n",
    "            # qtf = question.count(term) # SEPCIAL\n",
    "            qtf = 1.2\n",
    "            idf = log((total_docment - df + 0.5) / df + 0.5)\n",
    "            tf_Dt = ((k1 + 1) * tf[term][doc_id]) / (\n",
    "            k1 * ((1 - b) + b * (len(documents[doc_id]) / avg_doc_length) + tf[term][doc_id]))\n",
    "            if qtf == 0:\n",
    "                third = 0\n",
    "            else:\n",
    "                third = ((k3 + 1) * qtf) / (k3 + qtf)\n",
    "                okapibm25[term][doc_id] = idf * tf_Dt * third\n",
    "\n",
    "    return okapibm25\n",
    "\n",
    "\n",
    "# find top_k paragraph that may contain the answer\n",
    "def get_top_k_document(tfidf, query, k, document):\n",
    "    top_document_id = Counter()\n",
    "    for token in word_tokenize(query):\n",
    "        if token not in stopwordsAll:\n",
    "            term = lemmatizer.lemmatize(token.lower())\n",
    "            term_tfidf = tfidf[term]\n",
    "            for docid, weight in term_tfidf.items():\n",
    "                top_document_id[docid] += weight\n",
    "    top_document_id = top_document_id.most_common(k)\n",
    "    top_document = []\n",
    "    for document_id, weight in top_document_id:\n",
    "        top_document.append(document_id)\n",
    "    return top_document\n",
    "\n",
    "def prepare_test_del(train_corpus):\n",
    "    final_data = []\n",
    "    print(len(train_corpus))\n",
    "    for i in range(len(train_corpus)):\n",
    "        story = []\n",
    "        question = train_corpus[i]['question']\n",
    "        processed_question, answer, para_id, docid = qestion_and_answer(i)\n",
    "        document = get_paragraph(docid, docs_corpus)\n",
    "        tfs, total_docment, tfs_forward = term_freqs(document)\n",
    "        tfidf = get_okapibm25(tfs, total_docment, document)\n",
    "        top_1 = get_top_k_document(tfidf, question, 1, document)\n",
    "        for item in top_1:\n",
    "            # print(\"item\", item)\n",
    "            story = doc_to_story(item, docid)\n",
    "            # print(\"story\",story)\n",
    "        final_data.append((story, processed_question, answer))\n",
    "        # print(\"final_data\",final_data)\n",
    "    return final_data\n",
    "\n",
    "def prepare_test_data(test_corpus):\n",
    "    final_data = []\n",
    "    for item in test_corpus:\n",
    "        question = item['question']\n",
    "        docid = item['docid']\n",
    "        processed_question = word_tokenize(question)\n",
    "        document = get_paragraph(docid,docs_corpus)\n",
    "        tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "        tfidf = get_okapibm25(tfs, total_docment,document)\n",
    "        top_1 = get_top_k_document(tfidf,question,1,document)\n",
    "        for item in top_1:\n",
    "            #print(\"top_1\", top_1)\n",
    "            story = doc_to_story(item, docid)\n",
    "            #print(\"story\",story)\n",
    "        final_data.append((story, processed_question))\n",
    "        #print(\"final_data\",final_data)\n",
    "    return final_data\n",
    "\n",
    "del_data = prepare_test_del(test_for_del)\n",
    "print(\"len(del_data)\", len(del_data))\n",
    "\n",
    "def vectorize_stories_test(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    for story, query in data:\n",
    "        story_list = []\n",
    "        for sent in story:\n",
    "            for w in sent:\n",
    "                story_list.append(w)\n",
    "        x = [word_idx[w] for w in story_list]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen)\n",
    "\n",
    "print(\"len(test_for_predict)\",len(test_for_predict))\n",
    "test_data = prepare_test_data(test_for_predict)\n",
    "print(\"len(test_data)\",len(test_data))\n",
    "\n",
    "test_for_pre = dev_corpus[:200]\n",
    "prediction_data = prepare_test_data(test_for_pre)\n",
    "\n",
    "vocab = get_vocab(data,del_data,prediction_data)\n",
    "# print(\"vocab\",vocab)\n",
    "print(\"len(vocab)\", len(vocab))\n",
    "\n",
    "\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "WORD2VEC_EMBED_SIZE = 100\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "\n",
    "WORD2VEC_EMBED_SIZE = 100\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word2idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "idx2word = dict((i + 1, c) for i, c in enumerate(vocab))\n",
    "print(\"len(word2idx)\",len(word2idx))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in data)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in data)))\n",
    "\n",
    "x, xq, y = vectorize_stories(data, word2idx, story_maxlen, query_maxlen)\n",
    "tx, txq, ty = vectorize_stories(del_data, word2idx, story_maxlen, query_maxlen)\n",
    "\n",
    "print(\"Prepare test data...\")\n",
    "\n",
    "#test_story, test_question = vectorize_stories_test(test_data, word2idx, story_maxlen, query_maxlen)\n",
    "#test_question_data = [test_story, test_question]\n",
    "\n",
    "test_story_pre, test_question_pre = vectorize_stories_test(prediction_data, word2idx, story_maxlen, query_maxlen)\n",
    "test_question_pre = [test_story_pre, test_story_pre]\n",
    "\n",
    "del data\n",
    "del del_data\n",
    "gc.collect()\n",
    "\n",
    "#word2vec\n",
    "modelname = \"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/mywvmodel.model\"\n",
    "model = Word2Vec.load(modelname)  # 3个文件放在一起：Word60.model   Word60.model.syn0.npy   Word60.model.syn1neg.npy\n",
    "print(\"read model successful\")\n",
    "embedding_weights = np.zeros((vocab_size, WORD2VEC_EMBED_SIZE))\n",
    "for word, index in word2idx.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = model[word.lower()]\n",
    "    except KeyError:\n",
    "        pass  # keep as zero (not ideal, but what else can we do?)\n",
    "\n",
    "del model\n",
    "del word2idx\n",
    "gc.collect()\n",
    "\n",
    "#former\n",
    "\n",
    "# print('vocab = {}'.format(vocab))\n",
    "print('x.shape = {}'.format(x.shape))\n",
    "print('xq.shape = {}'.format(xq.shape))\n",
    "print('y.shape = {}'.format(y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\\'Build model...\\')\\n\\nqenc = Sequential()\\nqenc.add(Embedding(output_dim=WORD2VEC_EMBED_SIZE, input_dim=vocab_size,\\n                   weights=[embedding_weights], mask_zero=True))\\nqenc.add(LSTM(EMBED_HIDDEN_SIZE, input_shape=(None,story_maxlen), return_sequences=False))\\nqenc.add(Dropout(0.3))\\n\\naenc = Sequential()\\naenc.add(Embedding(output_dim=WORD2VEC_EMBED_SIZE, input_dim=vocab_size,\\n                   weights=[embedding_weights], mask_zero=True))\\naenc.add(LSTM(EMBED_HIDDEN_SIZE, input_shape=(None,query_maxlen), return_sequences=False))\\n#aenc.add(RepeatVector(story_maxlen))\\naenc.add(Dropout(0.3))\\n\\nmodel = Sequential()\\nmodel.add(Merge([qenc, aenc], mode=\"sum\"))\\nmodel.add(Dense(vocab_size, activation=\"softmax\"))\\n\\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\\n              metrics=[\"accuracy\"])\\n\\n\\n\\nprint(\\'Training\\')\\n\\nmodel.fit([x, xq], y,\\n          batch_size=BATCH_SIZE,\\n          epochs=EPOCHS,\\n          validation_split=0.05)\\nloss, acc = model.evaluate([tx, txq], ty,\\n                           batch_size=BATCH_SIZE)\\nprint(\\'Test loss / test accuracy = {:.4f} / {:.4f}\\'.format(loss, acc))\\n\\nprint(\"Saving model...\")\\n# serialize model to JSON\\nmodel_json = model.to_json()\\nwith open(\"predictor_model.json\", \"w\") as json_file:\\n    json_file.write(model_json)\\n# serialize weights to HDF5\\nmodel.save_weights(\"predictor_model.h5\")\\nprint(\"Saved model to disk\")\\n \\n# later...\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print('Build model...')\n",
    "\n",
    "qenc = Sequential()\n",
    "qenc.add(Embedding(output_dim=WORD2VEC_EMBED_SIZE, input_dim=vocab_size,\n",
    "                   weights=[embedding_weights], mask_zero=True))\n",
    "qenc.add(LSTM(EMBED_HIDDEN_SIZE, input_shape=(None,story_maxlen), return_sequences=False))\n",
    "qenc.add(Dropout(0.3))\n",
    "\n",
    "aenc = Sequential()\n",
    "aenc.add(Embedding(output_dim=WORD2VEC_EMBED_SIZE, input_dim=vocab_size,\n",
    "                   weights=[embedding_weights], mask_zero=True))\n",
    "aenc.add(LSTM(EMBED_HIDDEN_SIZE, input_shape=(None,query_maxlen), return_sequences=False))\n",
    "#aenc.add(RepeatVector(story_maxlen))\n",
    "aenc.add(Dropout(0.3))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([qenc, aenc], mode=\"sum\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "print('Training')\n",
    "\n",
    "model.fit([x, xq], y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.05)\n",
    "loss, acc = model.evaluate([tx, txq], ty,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n",
    "\n",
    "print(\"Saving model...\")\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"predictor_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"predictor_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimei/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1271: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\\'Testing...\\')\\noutputs = loaded_model.predict(test_question_data, batch_size = 1)\\nprint(\"type(outputs)\",type(outputs))\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('predictor_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"predictor_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "\n",
    "outputs = loaded_model.predict(test_question_pre, batch_size = 1)\n",
    "\n",
    "'''\n",
    "print('Testing...')\n",
    "outputs = loaded_model.predict(test_question_data, batch_size = 1)\n",
    "print(\"type(outputs)\",type(outputs))\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs [[9.4265751e-09 7.6395110e-05 9.4028241e-05 ... 9.4338599e-09\n",
      "  9.1478665e-09 9.8371205e-09]\n",
      " [3.1469949e-07 1.0241414e-05 2.2445070e-04 ... 3.1329145e-07\n",
      "  3.0461254e-07 3.2629580e-07]\n",
      " [7.4053497e-07 1.0680603e-03 4.9329636e-04 ... 7.3408154e-07\n",
      "  7.2627756e-07 7.5305229e-07]\n",
      " ...\n",
      " [1.1436958e-06 9.4233743e-05 1.2840941e-05 ... 1.1284994e-06\n",
      "  1.1328924e-06 1.1675074e-06]\n",
      " [2.1749463e-07 1.6511841e-04 3.1975785e-04 ... 2.1468661e-07\n",
      "  2.1066796e-07 2.2280359e-07]\n",
      " [3.2122493e-08 2.7648508e-05 2.4418243e-05 ... 3.2006021e-08\n",
      "  3.1178910e-08 3.3256946e-08]]\n",
      "story 1\n",
      "index 0 text1 salon\n",
      "story 1\n",
      "index 1 text1 purpose\n",
      "story 1\n",
      "index 2 text1 holocaust\n",
      "story 1\n",
      "story 1\n",
      "index 4 text1 23.2\n",
      "story 1\n",
      "index 5 text1 AOL\n",
      "story 1\n",
      "index 6 text1 morale\n",
      "story 1\n",
      "index 7 text1 salon\n",
      "story 1\n",
      "story 1\n",
      "index 9 text1 purpose\n",
      "story 1\n",
      "index 10 text1 salon\n",
      "story 1\n",
      "story 1\n",
      "index 12 text1 salon\n",
      "story 1\n",
      "index 13 text1 23.2\n",
      "story 1\n",
      "index 14 text1 morale\n",
      "story 1\n",
      "index 15 text1 salon\n",
      "story 1\n",
      "index 16 text1 holocaust\n",
      "story 1\n",
      "index 17 text1 extended-play\n",
      "story 1\n",
      "story 1\n",
      "index 19 text1 23.2\n",
      "story 1\n",
      "index 20 text1 purpose\n",
      "story 1\n",
      "index 21 text1 29°02′S\n",
      "story 1\n",
      "story 1\n",
      "index 23 text1 1985\n",
      "story 1\n",
      "index 24 text1 23.2\n",
      "story 1\n",
      "index 25 text1 2:00pm\n",
      "story 1\n",
      "index 26 text1 1985\n",
      "story 1\n",
      "index 27 text1 2:00pm\n",
      "story 1\n",
      "index 28 text1 29°02′S\n",
      "story 1\n",
      "index 29 text1 2C\n",
      "story 1\n",
      "index 30 text1 extended-play\n",
      "story 1\n",
      "index 31 text1 707\n",
      "story 1\n",
      "story 1\n",
      "index 33 text1 experiencing\n",
      "story 1\n",
      "index 34 text1 salon\n",
      "story 1\n",
      "index 35 text1 1970\n",
      "story 1\n",
      "index 36 text1 puruṣa\n",
      "story 1\n",
      "index 37 text1 2:00pm\n",
      "story 1\n",
      "index 38 text1 extended-play\n",
      "story 1\n",
      "index 39 text1 707\n",
      "story 1\n",
      "index 40 text1 salon\n",
      "story 1\n",
      "index 41 text1 1970\n",
      "story 1\n",
      "index 42 text1 229\n",
      "story 1\n",
      "index 43 text1 229\n",
      "story 1\n",
      "index 44 text1 AOL\n",
      "story 1\n",
      "index 45 text1 1985\n",
      "story 1\n",
      "index 46 text1 purpose\n",
      "story 1\n",
      "story 1\n",
      "index 48 text1 2:00pm\n",
      "story 1\n",
      "index 49 text1 2:00pm\n",
      "story 1\n",
      "index 50 text1 29°02′S\n",
      "story 1\n",
      "index 51 text1 experiencing\n",
      "story 1\n",
      "index 52 text1 salon\n",
      "story 1\n",
      "index 53 text1 purpose\n",
      "story 1\n",
      "story 1\n",
      "index 55 text1 23.2\n",
      "story 1\n",
      "index 56 text1 morale\n",
      "story 1\n",
      "index 57 text1 29°02′S\n",
      "story 1\n",
      "story 1\n",
      "index 59 text1 23.2\n",
      "story 1\n",
      "index 60 text1 AOL\n",
      "story 1\n",
      "index 61 text1 AOL\n",
      "story 1\n",
      "index 62 text1 extended-play\n",
      "story 1\n",
      "index 63 text1 salon\n",
      "story 1\n",
      "index 64 text1 extended-play\n",
      "story 1\n",
      "index 65 text1 23.2\n",
      "story 1\n",
      "index 66 text1 1985\n",
      "story 1\n",
      "index 67 text1 1970\n",
      "story 1\n",
      "story 1\n",
      "index 69 text1 extended-play\n",
      "story 1\n",
      "index 70 text1 1590\n",
      "story 1\n",
      "index 71 text1 AOL\n",
      "story 1\n",
      "index 72 text1 %\n",
      "story 1\n",
      "index 73 text1 holocaust\n",
      "story 1\n",
      "index 74 text1 %\n",
      "story 1\n",
      "index 75 text1 AOL\n",
      "story 1\n",
      "index 76 text1 AOL\n",
      "story 1\n",
      "index 77 text1 protestors\n",
      "story 1\n",
      "index 78 text1 29°02′S\n",
      "story 1\n",
      "index 79 text1 %\n",
      "story 1\n",
      "index 80 text1 2001–2011\n",
      "story 1\n",
      "index 81 text1 915,071\n",
      "story 1\n",
      "index 82 text1 salon\n",
      "story 1\n",
      "index 83 text1 protestors\n",
      "story 1\n",
      "index 84 text1 multi-purpose\n",
      "story 1\n",
      "index 85 text1 29°02′S\n",
      "story 1\n",
      "index 86 text1 AOL\n",
      "story 1\n",
      "index 87 text1 37mm\n",
      "story 1\n",
      "index 88 text1 devotees\n",
      "story 1\n",
      "story 1\n",
      "index 90 text1 hilltop\n",
      "story 1\n",
      "index 91 text1 witnesses—based\n",
      "story 1\n",
      "index 92 text1 AOL\n",
      "story 1\n",
      "index 93 text1 2001–2011\n",
      "story 1\n",
      "index 94 text1 devotees\n",
      "story 1\n",
      "index 95 text1 1819\n",
      "story 1\n",
      "index 96 text1 1819\n",
      "story 1\n",
      "index 97 text1 2002\n",
      "story 1\n",
      "index 98 text1 protestors\n",
      "story 1\n",
      "index 99 text1 salon\n",
      "story 1\n",
      "index 100 text1 %\n",
      "story 1\n",
      "index 101 text1 211\n",
      "story 1\n",
      "index 102 text1 roald\n",
      "story 1\n",
      "index 103 text1 AOL\n",
      "story 1\n",
      "index 104 text1 23.2\n",
      "story 1\n",
      "index 105 text1 20:18:04\n",
      "story 1\n",
      "index 106 text1 %\n",
      "story 1\n",
      "index 107 text1 2002\n",
      "story 1\n",
      "index 108 text1 mid-19th\n",
      "story 1\n",
      "index 109 text1 AOL\n",
      "story 1\n",
      "index 110 text1 witnesses—based\n",
      "story 1\n",
      "index 111 text1 publish\n",
      "story 1\n",
      "index 112 text1 2012-13\n",
      "story 1\n",
      "index 113 text1 %\n",
      "story 1\n",
      "index 114 text1 1590\n",
      "story 1\n",
      "index 115 text1 devotees\n",
      "story 1\n",
      "index 116 text1 mid-19th\n",
      "story 1\n",
      "index 117 text1 AOL\n",
      "story 1\n",
      "index 118 text1 AOL\n",
      "story 1\n",
      "index 119 text1 1675\n",
      "story 1\n",
      "index 120 text1 devotees\n",
      "story 1\n",
      "index 121 text1 %\n",
      "story 1\n",
      "index 122 text1 2012-13\n",
      "story 1\n",
      "index 123 text1 hilltop\n",
      "story 1\n",
      "index 124 text1 mid-19th\n",
      "story 1\n",
      "index 125 text1 %\n",
      "story 1\n",
      "index 126 text1 publish\n",
      "story 1\n",
      "index 127 text1 1819\n",
      "story 1\n",
      "index 128 text1 multi-purpose\n",
      "story 1\n",
      "index 129 text1 37mm\n",
      "story 1\n",
      "index 130 text1 1819\n",
      "story 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'argmax'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-4acd78ee93a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manswer_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mid_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mindex1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;31m#print(\"type(number)\",type(number))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m#print(\"number\",number)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \"\"\"\n\u001b[0;32m-> 1004\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from __future__ import unicode_literals\n",
    "from bs4 import BeautifulSoup\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"outputs\",outputs)\n",
    "\n",
    "def get_top_k_document_text(tfidf, query, k, document):\n",
    "    top_document_id = Counter()\n",
    "    for token in word_tokenize(query):\n",
    "        #print(\"token\",token)\n",
    "        if token not in stopwordsAll:\n",
    "            term = lemmatizer.lemmatize(token.lower())\n",
    "            #print(\"term\",term)\n",
    "            term_tfidf = tfidf[term]\n",
    "            #print(\"term_tfidf\",term_tfidf)\n",
    "            for docid, weight in term_tfidf.items():\n",
    "                top_document_id[docid] += weight\n",
    "    top_document_id = top_document_id.most_common(k)\n",
    "    #print(\"top_document_id\",top_document_id)\n",
    "    top_document = []\n",
    "    for document_id, weight in top_document_id:\n",
    "        top_document.append(document[document_id])\n",
    "        #print(\"top_document\",top_document)\n",
    "    if top_document == []:\n",
    "        print(\"top_document is null.\")\n",
    "        top_document.append(document[2])\n",
    "    #print(\"top_document\",top_document)\n",
    "    return top_document\n",
    "\n",
    "def prepare_process_data(test_corpus, index):\n",
    "    final_data = []\n",
    "    question = test_corpus[index]['question']\n",
    "    docid = test_corpus[index]['docid']\n",
    "    processed_question = word_tokenize(question)\n",
    "    document = get_paragraph(docid,docs_corpus)\n",
    "    tfs,total_docment,tfs_forward = term_freqs(document)\n",
    "    tfidf = get_okapibm25(tfs, total_docment,document)\n",
    "    top_1 = get_top_k_document_text(tfidf,question,1,document)\n",
    "    #print(\"top_1\",top_1)\n",
    "    final_data.append(top_1)\n",
    "    #print(\"final_data\",final_data)\n",
    "    return final_data\n",
    "\n",
    "count = 0\n",
    "predictions = []\n",
    "for index,answer_id in enumerate(outputs):\n",
    "    #print(index, answer_id)\n",
    "    try:\n",
    "        story = prepare_process_data(test_for_del,index)\n",
    "    except:\n",
    "        continue\n",
    "    print(\"story\",len(story[0]))\n",
    "    doc = \"\"\n",
    "    for sents in story:\n",
    "        #print(\"sents\",sents)\n",
    "        for sent in sents:\n",
    "            #print(\"sent\",sent)\n",
    "            doc = nlp(sent) \n",
    "    id_list = []\n",
    "    for item in answer_id:\n",
    "        id_list.append(item)\n",
    "    index1 = np.argmax(id_list)\n",
    "        #print(\"type(number)\",type(number))\n",
    "        #print(\"number\",number)\n",
    "    try:\n",
    "        text1 = idx2word[index1]\n",
    "    except:\n",
    "        continue\n",
    "    print(\"index\",index,\"text1\",text1)\n",
    "    answer_str = (index,text1)\n",
    "    predictions.append(answer_str)\n",
    "    count += 1\n",
    "print(\"count\",count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions[0] (0, 'salon')\n",
      "len(predictions) 187\n",
      "Finishing writing into output.csv!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "headers = ['id', 'answer']\n",
    "print(\"predictions[0]\",predictions[0])\n",
    "print(\"len(predictions)\",len(predictions))\n",
    "rows = predictions\n",
    "with open('output_zz.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerow(headers)\n",
    "    f_csv.writerows(rows)\n",
    "    \n",
    "print(\"Finishing writing into output.csv!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
