{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"Const\"\n",
      "op: \"Const\"\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"value\"\n",
      "  value {\n",
      "    tensor {\n",
      "      dtype: DT_FLOAT\n",
      "      tensor_shape {\n",
      "      }\n",
      "      float_val: 1.0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "graph = tf.get_default_graph()\n",
    "'''\n",
    "input_value = tf.constant(1.0)\n",
    "operations = graph.get_operations()\n",
    "print(operations[0].node_def)\n",
    "input_value\n",
    "sess = tf.Session()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_11\n",
      "Tensor(\"Variable_12/read:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Const_11:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one tensorflow  neuron\n",
    "'''\n",
    "weight = tf.Variable(0.8)\n",
    "#for op in graph.get_operations():\n",
    "    #print(op.name)\n",
    "out1 = weight*input_value\n",
    "op = graph.get_operations()[-1]\n",
    "print(op.name)\n",
    "for op_input in op.inputs: #tracks where the inputs come from\n",
    "    print(op_input)\n",
    "init = tf.initialize_all_variables() #initialize all the variables\n",
    "sess.run(init)\n",
    "sess.run(out1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "#See your graph in TensorBoard\n",
    "import tensorflow as tf\n",
    "graph = tf.get_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "x = tf.placeholder(\"float\",name=\"input_value\")\n",
    "w = tf.placeholder(\"float\",name=\"weight\")\n",
    "y = tf.multiply(x,w, name=\"out\")\n",
    "y_ = tf.placeholder(\"float\", name=\"true_value\")\n",
    "\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('log_simple_graph')\n",
    "summary_writer.add_graph(sess.graph)\n",
    "\n",
    "#loss = tf.square(tf.subtract(y, y_))\n",
    "# optim = tf.train.GradientDescentOptimizer(learning_rate=0.025)\n",
    "# grads_and_vars = optim.compute_gradients(loss)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "print(sess.run(y,feed_dict={x:10.0,w:1.0,y_:9.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QA system\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntestwords = ['policy','energy','heat']\\n\\nfor i in range(len(testwords)):\\n    res = model.most_similar(testwords[i])\\n    print(testwords[i])\\n    print(res)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def loadEmbedding(filename):\n",
    "    \"\"\"\n",
    "    加载词向量文件\n",
    "    :param filename: 文件名\n",
    "    :return: embeddings列表和它对应的索引\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    word2idx = defaultdict(list)\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-8\") as rf:\n",
    "        for line in rf:\n",
    "            arr = line.split(\" \")\n",
    "            embedding = [float(val) for val in arr[1: -1]]\n",
    "            word2idx[arr[0]] = len(word2idx)\n",
    "            embeddings.append(embedding)\n",
    "    return embeddings, word2idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/documents.json\",'r') as docs:\n",
    "    data = json.load(docs)\n",
    "    for i in range(len(data)):\n",
    "        for para in data[i]['text']:\n",
    "            sents = sent_tokenize(para)\n",
    "            #print(\"sents\",sents)\n",
    "            for sent in sents:\n",
    "                tokens = word_tokenize(sent)\n",
    "                #print(\"tokens\",tokens)\n",
    "                corpus.append(tokens)\n",
    "               \n",
    "#model = Word2Vec(corpus,min_count=1)\n",
    "#model.save(modelname)\n",
    "'''\n",
    "testwords = ['policy','energy','heat']\n",
    "\n",
    "for i in range(len(testwords)):\n",
    "    res = model.most_similar(testwords[i])\n",
    "    print(testwords[i])\n",
    "    print(res)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.020241  ,  0.2772321 ,  1.2114432 ,  0.4887277 ,  1.4936604 ,\n",
       "       -0.96980697, -0.14549977,  0.7126327 ,  0.01868612,  1.2887446 ,\n",
       "        0.29550806,  0.04738949, -0.72028744, -0.34642467,  0.47582132,\n",
       "        0.83327293, -0.29757446, -0.73650646, -0.09524413, -1.1236236 ,\n",
       "       -0.4855214 ,  0.5764676 , -0.02008057,  0.9259296 ,  0.11188792,\n",
       "        0.49843925, -0.05781586,  1.3234041 ,  0.29324487,  0.86236155,\n",
       "       -0.34498906,  1.1730989 ,  0.71607524, -0.684642  , -0.0319341 ,\n",
       "        0.19619343, -0.5773926 , -0.946204  ,  0.20814006, -0.35979575,\n",
       "       -0.22039971, -0.03355778, -0.37396216, -0.6717335 ,  0.2355556 ,\n",
       "        0.31127143,  0.46387553,  0.0289211 , -0.53886116, -0.4195549 ,\n",
       "        0.13438214,  0.28202483, -1.7682979 ,  0.48850814, -0.28944263,\n",
       "        0.9149001 ,  0.7584148 , -0.18706481, -0.39843526, -0.39688042,\n",
       "       -0.8806692 ,  0.96281797,  1.2962068 ,  0.56937385,  1.5201175 ,\n",
       "        0.10478542, -0.33571494, -1.5408399 ,  0.42653286,  0.67174983,\n",
       "        0.14903359, -0.29088372, -0.08261019,  0.7078401 ,  0.36457992,\n",
       "       -0.48995498, -0.96977395,  0.11598067, -0.466494  ,  0.22105844,\n",
       "       -0.19291875, -0.39770278, -0.4821974 , -0.17729999,  0.59366786,\n",
       "       -0.47440115, -0.1874004 ,  0.56707805, -0.4394386 ,  0.65117526,\n",
       "        0.10981774, -0.5510935 ,  0.29767916,  0.71571416,  1.4846708 ,\n",
       "       -0.34314838, -0.6050041 ,  0.1944571 , -0.78622687, -0.3552591 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec.load(modelname)\n",
    "#model.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3618\n",
      "line 0,None\n",
      "\n",
      "['0', 'None\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimei/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_current_question [[('Modern', 'NNP'), ('browser', 'NN'), ('support', 'NN'), ('standards-based', 'JJ'), ('and', 'CC'), ('defacto', 'JJ'), ('what', 'WP'), ('?', '.')]]\n",
      "line 1,None\n",
      "\n",
      "['1', 'None\\n']\n",
      "POS_current_question [[('What', 'WP'), ('do', 'VBP'), ('people', 'NNS'), ('typically', 'RB'), ('call', 'VBP'), ('a', 'DT'), ('web', 'NN'), ('browser', 'NN'), ('?', '.')]]\n",
      "line 2,None\n",
      "\n",
      "['2', 'None\\n']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4b4bdac2592c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mcurrent_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_questions_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mPOS_current_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhat_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POS_current_question\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPOS_current_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Run the tagger and get the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,\n\u001b[0;32m--> 107\u001b[0;31m                                        stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mstanpos_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanpos_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# Check the return code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1512\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "jar1 = '/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/stanford-postagger-2018-02-27/stanford-postagger.jar'\n",
    "model3 = '/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/stanford-postagger-2018-02-27/models/english-bidirectional-distsim.tagger'\n",
    "\n",
    "all_questions_dict = {}\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/testing.json\",'r') as test_file:\n",
    "    test_data = json.load(test_file)\n",
    "    for i in range(len(test_data)):\n",
    "        question = test_data[i]['question']\n",
    "        all_questions_dict[i] = question\n",
    "print(len(all_questions_dict))\n",
    " \n",
    "def get_POS_tag_model(model,jar):\n",
    "    return StanfordPOSTagger(model,jar)\n",
    "\n",
    "what_model = get_POS_tag_model(model3, jar1)\n",
    "    \n",
    "with open('out.txt', 'rt') as f:\n",
    "    for line in f:\n",
    "        print(\"line\",line)\n",
    "        item = line.split(',')\n",
    "        print(item)\n",
    "        if item[1].startswith(\"None\"):\n",
    "            current_question = all_questions_dict.get(int(item[0]), 0)\n",
    "            POS_current_question = what_model.tag_sents([[\" \".join(x) for x in [word_tokenize(current_question)]]])\n",
    "            print(\"POS_current_question\",POS_current_question)\n",
    "        else:\n",
    "            continue\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(corpus1) 93515\n",
      "corpus1[0] ['First', 'recognized', 'in', '1900', 'by', 'Max', 'Planck', ',', 'it', 'was', 'originally', 'the', 'proportionality', 'constant', 'between', 'the', 'minimal', 'increment', 'of', 'energy', ',', 'E', ',', 'of', 'a', 'hypothetical', 'electrically', 'charged', 'oscillator', 'in', 'a', 'cavity', 'that', 'contained', 'black', 'body', 'radiation', ',', 'and', 'the', 'frequency', ',', 'f', ',', 'of', 'its', 'associated', 'electromagnetic', 'wave', '.']\n",
      "len(corpus2) 86758\n",
      "corpus2[0] ['A', 'kilogram', 'could', 'be', 'definined', 'as', 'having', 'a', 'Planck', 'constant', 'of', 'what', 'value', '?']\n",
      "len(corpus3) 3618\n",
      "corpus3[0] ['Modern', 'browser', 'support', 'standards-based', 'and', 'defacto', 'what', '?']\n",
      "[ 3.72187108e-01  3.56974155e-01 -3.37382369e-02 -1.56308368e-01\n",
      " -1.09161127e+00 -3.73485863e-01  2.60382235e-01 -3.78182828e-01\n",
      "  1.51048911e+00 -1.83296347e+00  1.34392772e-02  5.04885495e-01\n",
      "  9.55076098e-01  9.54042077e-02 -2.64968991e-01  8.92491639e-01\n",
      "  1.61401498e+00 -1.14955425e-01 -2.78254420e-01  1.90464392e-01\n",
      " -1.22148073e+00 -5.01127899e-01 -4.01208133e-01 -1.10080099e+00\n",
      " -7.94049859e-01 -1.07159412e+00 -9.31238413e-01 -6.61014378e-01\n",
      " -1.88368773e+00  6.98581219e-01 -1.33374286e+00  1.03292322e+00\n",
      " -4.29511964e-01  6.42306507e-01  6.72307253e-01  2.22488105e-01\n",
      " -9.28929389e-01  1.20802259e+00  5.91597676e-01 -1.35831505e-01\n",
      "  3.07646662e-01  3.84273529e-02  4.94017184e-01  8.95145118e-01\n",
      "  1.65406406e+00 -7.48359501e-01 -1.19548273e+00 -1.05081685e-01\n",
      " -3.02362710e-01 -7.44080007e-01 -1.15519869e+00 -7.84421921e-01\n",
      " -1.38234198e-01  5.09766698e-01 -6.83397725e-02 -3.62444460e-01\n",
      "  7.80433893e-01  6.63545012e-01 -6.56062007e-01 -1.33840322e+00\n",
      "  7.55829155e-01  1.29172251e-01 -1.53342974e+00 -7.19275773e-01\n",
      "  3.31348002e-01 -3.48505765e-01 -6.17843330e-01  1.33088815e+00\n",
      "  3.37933391e-01 -8.43046963e-01  3.57866466e-01  1.43859710e-03\n",
      "  4.08496916e-01 -1.64608032e-01  1.42974816e-02  5.24854660e-01\n",
      " -6.46328270e-01 -7.23321438e-01 -5.66520095e-01 -9.90301669e-01\n",
      " -7.37071037e-01  2.01184109e-01  4.60327804e-01 -7.96423331e-02\n",
      "  1.18293548e+00  4.18854840e-02  4.55320448e-01 -1.03101802e+00\n",
      " -1.01615870e+00  6.09889567e-01 -1.91174030e-01  5.07991672e-01\n",
      "  4.99089599e-01  4.86057699e-01 -1.54403460e+00  2.52080113e-01\n",
      "  4.54020388e-02 -7.14634001e-01  9.95656550e-02 -5.72149634e-01]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "corpus1 = []\n",
    "docs_corpus = []\n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/documents.json\",'r') as docs:\n",
    "    docs_corpus = json.load(docs)\n",
    "    for i in range(len(docs_corpus)):\n",
    "        for para in docs_corpus[i]['text']:\n",
    "            sents = sent_tokenize(para)\n",
    "            #print(\"sents\",sents)\n",
    "            for sent in sents:\n",
    "                tokens = word_tokenize(sent)\n",
    "                #print(\"tokens\",tokens)\n",
    "                corpus1.append(tokens)\n",
    "print(\"len(corpus1)\",len(corpus1))\n",
    "print(\"corpus1[0]\",corpus1[0])\n",
    "\n",
    "corpus2 = []   \n",
    "train_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/training.json\",'r') as training:\n",
    "    train_corpus = json.load(training)\n",
    "    #print(\"len(train_corpus)\",len(train_corpus))\n",
    "    for i in range(len(train_corpus)):\n",
    "        question = train_corpus[i]['question']\n",
    "        answer = train_corpus[i]['text']\n",
    "        processed_question = word_tokenize(question)\n",
    "        corpus2.append(processed_question)\n",
    "        processed_answer = word_tokenize(answer)\n",
    "        corpus2.append(processed_answer)\n",
    "print(\"len(corpus2)\",len(corpus2))\n",
    "print(\"corpus2[0]\",corpus2[0])\n",
    "\n",
    "          \n",
    "\n",
    "#test_for_train = train_corpus[:10000]\n",
    "\n",
    "dev_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/devel.json\",'r') as devel:\n",
    "    dev_corpus = json.load(devel)\n",
    "    \n",
    "#test_for_del = dev_corpus[:1000]\n",
    "\n",
    "corpus3 = []\n",
    "test_corpus = []             \n",
    "with open(\"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/testing.json\",'r') as test:\n",
    "    test_corpus = json.load(test)\n",
    "    for i in range(len(test_corpus)):\n",
    "        question = test_corpus[i]['question']\n",
    "        processed_question = word_tokenize(question)\n",
    "        corpus3.append(processed_question)\n",
    "print(\"len(corpus3)\",len(corpus3))\n",
    "print(\"corpus3[0]\",corpus3[0])\n",
    "\n",
    "modelname = \"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/mymodel.bin.gz\"\n",
    "\n",
    "sentences = corpus1 + corpus2 + corpus3\n",
    "model = Word2Vec(window=5, min_count=1)\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.save(modelname)\n",
    "print(model.wv['computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
