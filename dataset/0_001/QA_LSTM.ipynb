{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimei/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/shimei/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Merge, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD2VEC_EMBED_SIZE = 300\n",
    "QA_EMBED_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "NBR_EPOCHS = 40\n",
    "modelname = \"/Users/shimei/Documents/2018/Web_Search/homework/Final_Assignment/project_files/mymodel.bin.gz\"\n",
    "model = Word2Vec.load(modelname)\n",
    "print(\"Loading Word2Vec model and generating embedding matrix...\")\n",
    "print(model.wv['computer'])\n",
    "\n",
    "print(\"Loading and formatting data...\")\n",
    "\n",
    "def get_question_answer_pairs(question_file, is_test=False):\n",
    "    qapairs = []\n",
    "    fqa = open(question_file, \"rb\")\n",
    "    for line in fqa:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        line = line.strip().decode(\"utf8\").encode(\"ascii\", \"ignore\")\n",
    "        cols = line.split(\"\\t\")\n",
    "        question = cols[1]\n",
    "        qwords = nltk.word_tokenize(question)\n",
    "        if not is_test:\n",
    "            correct_ans = cols[2]\n",
    "            answers = cols[3:]\n",
    "            # training file parsing\n",
    "            correct_ans_idx = ord(correct_ans) - ord('A')\n",
    "            for idx, answer in enumerate(answers):\n",
    "                awords = nltk.word_tokenize(answer)\n",
    "                qapairs.append((qwords, awords, idx == correct_ans_idx))\n",
    "        else:\n",
    "            # test file parsing (no correct answer)\n",
    "            answers = cols[2:]\n",
    "            for answer in answers:\n",
    "                awords = nltk.word_tokenize(answer)\n",
    "                qapairs.append((qwords, awords, None))\n",
    "    fqa.close()\n",
    "    return qapairs\n",
    "\n",
    "qapairs = kaggle.get_question_answer_pairs(\n",
    "    os.path.join(DATA_DIR, QA_TRAIN_FILE))\n",
    "question_maxlen = max([len(qapair[0]) for qapair in qapairs])\n",
    "answer_maxlen = max([len(qapair[1]) for qapair in qapairs])\n",
    "seq_maxlen = max([question_maxlen, answer_maxlen])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
